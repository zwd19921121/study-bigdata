
# 用户行为采集平台
## 1 数仓概念

数据仓库(Data Warehouse)，是为企业所有决策制定过程，提供所有系统数据的支持的战略集合。

通过对数据仓库中的数据分析，可以帮助企业，改进业务流程、控制陈本、提高产品质量等。

数据仓库，并不是数据的最终目的地，而是为数据最终的目的地做好准备。这些准备包括对数据的：清洗、转义、分类、重组、合并、拆分、统计等等。

![数据仓库](https://gitee.com/zhdoop/blogImg/raw/master/img/数据仓库.png)

## 2 项目需求及架构设计

### 2.1 项目需求分析

#### 2.2.1 项目需求

1. 用户行为数据采集平台搭建
2. 业务数据采集平台搭建
3. 数据仓库维度建模
4. 分析，设备、会员、商品、地区、活动等电商核心主题，统计的报表指标近100个。完全对比中型公司
5. 采用即席查询工具，随时进行指标分析
6. 对集群的性能进行监控，发生异常需要报警
7. 元数据管理
8. 质量监控

#### 2.2.2 思考题

1. 项目技术如何选型
2. 框架版本如何选型（Apache、CDH、HDP)
3. 服务器使用物理机还是云主机
4. 如何确认集群规模（假设每台服务器8T硬盘）

### 2.2 项目框架

#### 2.2.1 技术选型

技术选型主要考虑的因素：数据量的大小、业务需求、行业内经验、技术成熟度、开发维护成本、总成本预算

* 数据采集传输：<font color=red>Flume</font>,<font color=red>Kafka</font>,<font color=red>Sqoop</font>,Logstash,DataX
* 数据存储：<font color=red>Mysql</font>,<font color=red>HDFS</font>,HBase,Redis,MongoDB
* 数据计算：<font color=red>Hive</font>,<font color=red>Tez</font>,<font color=red>Spark</font>,Flink,Storm
* 数据查询：<font color=red>Presto</font>,<font color=red>Kylin</font>,Impala,Druid
* 数据可视化：Echarts,<font color=red>Superset</font>,QuickBI,DataV
* 任务调度：<font color=red>Azkaban</font>、Oozie
* 集群监控：<font color=red>Zabbix</font>
* 元数据管理：<font color=red>Atlas</font>

#### 2.2.2 系统数据流设计

![离线数仓系统数据流程设计](https://gitee.com/zhdoop/blogImg/raw/master/img/离线数仓系统数据流程设计.png)

#### 2.2.3 框架版本选型

##### 2.2.3.1 如何选择Apache、CDH、HDP版本

1. Apache

   运维麻烦，组件间兼容性需要自己调研。（一般大厂使用，技术实力雄厚，有专门的运维人员，建议使用）

2. CDH

   国内使用最多的版本，但是CM不开源，开始收费，一个节点1万美金。

3. HDP

   开源，可以进行二次开发，但是没有CDH稳定，国内使用较少

##### 2.2.3.2 Apache框架选择

| 框架        | 旧版本   | 新版本      |
| ----------- | -------- | ----------- |
| Hadoop      | 2.7.2    | 3.1.3       |
| Zookeeper   | 3.4.10   | 3.5.7       |
| MySQL       | 5.6.24   | 5.7.16      |
| Hive        | 1.2.1    | 3.1.2       |
| Flume       | 1.7.0    | 1.9.0       |
| Kafka       | 0.11-0.2 | _2.11-2.4.1 |
| Kafka Eagle | 1.3.7    | 1.4.5       |
| Azkaban     | 2.5.0    | 3.84.4      |
| Spark       | 2.1.1    | 3.0.0       |
| Hbase       | 1.3.1    | 2.0.5       |
| Phoenix     | 4.14.1   | 5.0.0       |
| Sqoop       | 1.4.6    |             |
| Presto      | 0.189    |             |
| Kylin       | 2.5.1    | 3.0.1       |
| Atlas       | 0.8.4    | 2.0.0       |
| Ranger      | 2.0.0    |             |
| Solr        | 5.2.1    | 7.7.0       |

#### 2.2.4 服务器选型

服务器选择物理机还是云主机？

##### 2.2.4.1 物理机：

* 以128G内存，20核物理CPU，40线程，8THDD和2TSSD硬盘，戴尔品牌

  单台报价4W出头。一半物理机寿命5年左右

* 需要专业的运维人员，平均一个月1万。电费也是不小的开销

##### 2.2.4.2 云主机

* 云主机：以阿里云为例，差不多相同的配置，每年5w
* 很多运维工作都由阿里云完成，运维相对轻松

##### 2.2.4.3 企业选择

* 金融有钱公司和阿里没有直接冲突的公司选择阿里云
* 中小公司、为了融资上市，选择阿里云，拉到融资后买物理机
* 有长期打算，资金比较充足，选择物理机

#### 2.2.5 集群资源规划设计

##### 2.2.5.1 如何确认集群规模（假设：每台服务器8T磁盘，128G内存）

1. 每天日活用户100万，没人一天平均100条：<font color=red>100万*100=1亿条</font>
2. 每条日志1K左右，每天一亿条：<font color=red>100000000/1024/1024≈100G</font>
3. 半年内不扩容服务器来算：<font color=red>100G*180≈18T</font>
4. 保存3个副本：<font color=red>18T*3=54T</font>
5. 预留20%~30%Buf：<font color=red>54T/0.7=77T</font>
6. 算到这：<font color=red>约8T*10台服务器</font>
7. 数仓多分层，在当前基础上在扩展1到2倍：<font color=red>约8Tx10台x2|3=20~30台</font>

如果考虑数仓分层？数据采用压缩？需要重新再计算

采用snappy或者lzo 压缩率在60%左右

##### 2.2.5.2 测试集群服务器规划

整体原则

* 资源均衡
* 有依赖的服务需要在同一个节点上，例如Azkanba 的Executor调度的hive或者Sqoop，需要在一个节点上

<table border="1" align="center">
    <tr>
        <th>服务名称</th>
        <th>子服务</th>
        <th>服务器node1</th>
        <th>服务器node2</th>
        <th>服务器node3</th>
    </tr>
    <!-- HDFS 开始 -->
     <tr>
        <td rowspan="3" width=150 height=30>HDFS </td>
        <td rowspan="1" width=200 height=30>NameNode</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <tr>
        <td rowspan="1" width=200 height=30>DataNode</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
    </tr>
     <tr>
        <td rowspan="1" width=200 height=30>SecondaryNameNode</td>
        <td  height=30></td>
        <td  height=30></td>
        <td  height=30>√</td>
    </tr>
    <!-- HDFS 结束 -->
     <!-- Yarn 开始 -->
     <tr>
        <td rowspan="2" width=150 height=30>Yarn </td>
        <td rowspan="1" width=200 height=30>NodeManager</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
    </tr>
    <tr>
        <td rowspan="1" width=200 height=30>ResourceManager</td>
        <td  height=30></td>
        <td  height=30>√</td>
        <td  height=30></td>
    </tr>
    <!-- Yarn 结束 -->
     <!-- Zookeeper 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Yarn </td>
        <td rowspan="1" width=200 height=30>NodeManager</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
    </tr>
    <!-- Zookeeper 结束 -->
    <!-- Flume(采集日志) 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Flume(采集日志) </td>
        <td rowspan="1" width=200 height=30>Flume</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
        <td  height=30></td>
    </tr>
    <!-- Flume(采集日志) 结束 -->
    <!-- Kafka 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Kafka </td>
        <td rowspan="1" width=200 height=30>Kafka</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
    </tr>
    <!-- kafka 结束 -->
    <!-- Flume(消费Kafka) 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Flume(消费Kafka) </td>
        <td rowspan="1" width=200 height=30>Flume</td>
        <td  height=30></td>
        <td  height=30></td>
        <td  height=30>√</td>
    </tr>
    <!-- Flume(消费Kafka) 结束 -->
    <!-- Hive 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Flume(消费Kafka) </td>
        <td rowspan="1" width=200 height=30>Flume</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <!-- Hive 结束 -->
    <!-- Hive 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>MySQL </td>
        <td rowspan="1" width=200 height=30>MySQL</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <!-- Hive 结束 -->
    <!-- Sqoop开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Sqoop </td>
        <td rowspan="1" width=200 height=30>Sqoop</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <!-- Sqoop 结束 -->
    <!-- Presto 开始 -->
     <tr>
        <td rowspan="2" width=150 height=30>Sqoop </td>
        <td rowspan="1" width=200 height=30>Coordinator</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
     <tr>
        <td rowspan="1" width=200 height=30>Worker</td>
        <td  height=30></td>
        <td  height=30>√</td>
        <td  height=30>√</td>
    </tr>
    <!-- Presto 结束 -->
    <!-- Azkaban 开始 -->
     <tr>
        <td rowspan="2" width=150 height=30>Azkaban</td>
        <td rowspan="1" width=200 height=30>AzkabanWebServer</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
     <tr>
        <td rowspan="1" width=200 height=30>AzkabanExecutorServer</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <!-- Azkaban 结束 -->
    <!-- Kylin 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Kylin></td>
        <td rowspan="1" width=200 height=30></td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <!-- Kylin 结束 -->
    <!-- HBase 开始 -->
     <tr>
        <td rowspan="2" width=150 height=30>HBase </td>
        <td rowspan="1" width=200 height=30>HMaster</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
     <tr>
        <td rowspan="1" width=200 height=30>HRegionServer</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
        <td  height=30>√</td>
    </tr>
    <!-- HBase 结束 -->
    <!-- Superset 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Superset </td>
        <td rowspan="1" width=200 height=30></td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <!-- Superset 结束 -->
    <!-- Atlas 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Atlas </td>
        <td rowspan="1" width=200 height=30></td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <!-- Atlas 结束 -->
    <!-- Solr 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>Solr </td>
        <td rowspan="1" width=200 height=30>Jar</td>
        <td  height=30>√</td>
        <td  height=30></td>
        <td  height=30></td>
    </tr>
    <!-- Solr 结束 -->
    <!-- 服务数总计 开始 -->
     <tr>
        <td rowspan="1" width=150 height=30>服务数总计 </td>
        <td rowspan="1" width=200 height=30></td>
        <td  height=30>18</td>
        <td  height=30>8</td>
        <td  height=30>8</td>
    </tr>
    <!-- 服务数总计 结束 -->
</table>


## 3 数据生成模块

### 3.1 目标数据

我们要收集和分析的数据主要包括页面数据、事件数据、曝光数据、启动数据和错误数据。

#### 3.1.1 页面

页面数据主要记录一个页面用户访问情况，包括访问时间、停留时间、页面路径等信息。

![页面数据](https://gitee.com/zhdoop/blogImg/raw/master/img/页面.PNG)

<table border="1" align="center">
    <tr>
        <th>字段名称</th>
        <th>字段描述</th>
    </tr>
     <tr>
        <td rowspan="1" width=250 height=30>page_id</td>
        <td rowspan="1" height=30>页面id <br/><font color=red>home("页面")</font>，<br/><font color=red>category("分类页")</font>,<br/><font color=red>discovery("发现页")</font>,<br/><font color=red>top_n("热门排行")</font>,<br/><font color=red>favor("收藏页")</font>，<br/><font color=red>search("搜索页")</font>,<br/><font color=red>good_list("商品列表页")</font>,<br/><font color=red>good_detail("商品详情页")</font>,<br/><font color=red>good_spec("商品规格")</font>，<br/><font color=red>comment("评价")</font>,<br/><font color=red>comment_done("评价完成")</font>,<br/><font color=red>comment_list("评价列表")</font>,<br/><font color=red>cart("购物车")</font>，<br/><font color=red>trade("下单结算")</font>,<br/><font color=red>payment("支付页面")</font>,<br/><font color=red>payment_done("支付完成")</font>,<br/><font color=red>orders_all("全部订单")</font>，<br/><font color=red>orders_unpaid("订单待支付")</font>,<br/><font color=red>orders_undelivered("订单待发货")</font>,<br/><font color=red>orders_unreceipted("订单待收货")</font>,<br/><font color=red>order_wait_comment("订单待评价")</font>，<br/><font color=red>mine("我的")</font>,<br/><font color=red>activity("活动")</font>,<br/><font color=red>login("登录")</font>,<br/><font color=red>register("注册")</font></td>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>page_id</td>
       <td rowspan="1" height=30>上页id</td>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>page_item_type</td>
       <td rowspan="1" height=30>页面对象类型<br/><font color=red>sku_id("商品 skuId")</font>，<br/><font color=red>keyword("搜索关键字")</font>,<br/><font color=red>sku_ids("多个商品 skuId")</font>,<br/><font color=red>activity_id("活动 id")</font>,<br/><font color=red>coupon_id("购物劵 id")</font></td>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>page_item</td>
       <td rowspan="1" height=30>页面对象id</td>
    </tr>
     <tr>
       <td rowspan="1" width=250 height=30>sourceType</td>
       <td rowspan="1" height=30>页面来源类型<br/><font color=red>promotion("商品推广")</font>,<br/><font color=red>recommend("算法推荐商品")</font>,<br/><font color=red>query("查询结果商品")</font>,<br/><font color=red>activity("促销活动")</font></td>
    </tr>
     <tr>
       <td rowspan="1" width=250 height=30>during_time</td>
       <td rowspan="1" height=30>停留时间（毫秒）</td>
    </tr>
     <tr>
       <td rowspan="1" width=250 height=30>ts</td>
       <td rowspan="1" height=30>跳入时间</td>
    </tr>
</table>

#### 3.1.2 事件

事件数据主要记录应用内一个具体操作行为，包括操作类型、操作对象、操作对象描述等信息。

![事件数据](https://gitee.com/zhdoop/blogImg/raw/master/img/事件PNG.PNG)

<table border="1" align="center">
    <tr>
        <th>字段名称</th>
        <th>字段描述</th>
    </tr>
     <tr>
        <td rowspan="1" width=250 height=30>action_id</td>
        <td rowspan="1" height=30>动作id <br/><font color=red>favor_add("添加收藏")</font>，<br/><font color=red>favor_cancel("取消收藏")</font>,<br/><font color=red>cart_add("添加购物车")</font>,<br/><font color=red>cart_remove("删除购物车")</font>,<br/><font color=red>cart_add_num("增加购物车商品数量")</font>，<br/><font color=red>cart_minus_num("减少购物车的数量")</font>,<br/><font color=red>trade_add_address("增加收货地址")</font>,<br/><font color=red>get_coupon("领取优惠券")</font>/td>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>item_type</td>
       <td rowspan="1" height=30>动作目标类型<br/><font color=red>sku_id("商品")</font>，<br/><font color=red>coupon_id("购物券")</font></td>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>item</td>
       <td rowspan="1" height=30>动作目标id</td>
    </tr>
     <tr>
       <td rowspan="1" width=250 height=30>ts</td>
       <td rowspan="1" height=30>跳入时间</td>
    </tr>
</table>

#### 3.1.3 曝光

曝光数据主要记录页面所曝光的内容，包括曝光对象、曝光类型等信息。

![曝光数据](https://gitee.com/zhdoop/blogImg/raw/master/img/曝光.PNG)

<table border="1" align="center">
    <tr>
        <th>字段名称</th>
        <th>字段描述</th>
    </tr>
     <tr>
        <td rowspan="1" width=250 height=30>displayType</td>
        <td rowspan="1" height=30>曝光类型 <br/><font color=red>promotion("商品推广")</font>，<br/><font color=red>recommand("算法推荐商品")</font>,<br/><font color=red>query("查询结果商品")</font>,<br/><font color=red>activity("促销活动")</font>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>item_type</td>
       <td rowspan="1" height=30>曝光对象类型<br/><font color=red>sku_id("商品 skuId")</font>，<br/><font color=red>activity_id("活动id")</font></td>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>item</td>
       <td rowspan="1" height=30>曝光对象id</td>
    </tr>
     <tr>
       <td rowspan="1" width=250 height=30>order</td>
       <td rowspan="1" height=30>曝光顺序</td>
    </tr>
</table>

#### 3.1.4 启动

启动数据记录应用的启动信息。

![启动数据](https://gitee.com/zhdoop/blogImg/raw/master/img/启动.PNG)

<table border="1" align="center">
    <tr>
        <th>字段名称</th>
        <th>字段描述</th>
    </tr>
     <tr>
        <td rowspan="1" width=250 height=30>entry</td>
        <td rowspan="1" height=30>启动入口 <br/>icon("图标")<br/>notification("通知")<br/>install("安装后启动")</font>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>loading_time</td>
       <td rowspan="1" height=30>启动加载时间</td>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>open_add_id</td>
       <td rowspan="1" height=30>开屏广告id</td>
    </tr>
     <tr>
       <td rowspan="1" width=250 height=30>open_ad_ms</td>
       <td rowspan="1" height=30>广告播放时间</td>
    </tr>
     <tr>
       <td rowspan="1" width=250 height=30>open_ad_skip_ms</td>
       <td rowspan="1" height=30>用户跳过广告时间</td>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>ts</td>
       <td rowspan="1" height=30>启动时间</td>
    </tr>
</table>

#### 3.1.5 错误

错误数据记录应用使用过程中的错误信息，包括错误编号及错误信息。

<table border="1" align="center">
    <tr>
        <th>字段名称</th>
        <th>字段描述</th>
    </tr>
     <tr>
        <td rowspan="1" width=250 height=30>erroer_code</td>
        <td rowspan="1" height=30>错误码</font>
    </tr>
    <tr>
       <td rowspan="1" width=250 height=30>msg</td>
       <td rowspan="1" height=30>错误信息</td>
    </tr>
</table>

### 3.2 数据埋点

#### 3.2.1 主流埋点方式

目前主流的埋点方式，有代码埋点（前端/后端）、可视化埋点、全埋点三种。

<font color=red>代码埋点</font>时通过调用埋点SDK函数，在需要埋点的业务逻辑功能位置调用接口，上报埋点数据。例如，我们对页面中的某个按钮埋点后，当这个按钮被点击时，可以在这个按钮对应的OnClick 函数里调用SDK提供的数据发送接口，来发送数据。

<font color=red>可视化埋点</font>只需要研发人员集成采集SDK，不需要写埋点代码，业务人员就可以通过分析平台的"圈选"功能，来圈出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集SDK按照圈选的配置自动进行用户行为数据的采集和发送。

<font color=red>全埋点</font>是通过产品中嵌入SDK，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点。然后再通过界面配置哪些数据需要在系统里面进行分析。

#### 3.2.2 埋点数据日志结构

我们的日志结构大致分为两类，一是普通页面埋点日志，而是启动日志。

普通页面日志结构如下，每条日志包含了，当前页面的页面信息，所有事件（动作）、所有曝光信息以及错误信息。除此之外，还包含了一系列公共信息，包括设备信息，地理位置，应用信息等，即下边的common字段。

1. 普通页面埋点日志格式

   ```json
   {
       "common": { -- 公共信息
            "ar": "230000", -- 地区编码
           "ba": "iPhone", -- 手机品牌
           "ch": "Appstore", -- 渠道
           "md": "iPhone 8", -- 手机型号
           "mid": "YXfhjAYH6As2z9Iq", -- 设备 id
           "os": "iOS 13.2.9", -- 操作系统
           "uid": "485", -- 会员 id
           "vc": "v2.1.134" -- app 版本号
        },
       "actions": [ --动作(事件)
           {
             "action_id": "favor_add", --动作 id
             "item": "3", --动作目标 id
             "item_type": "sku_id", --动作目标类型
             "ts": 1585744376605 --动作时间
            }
         ],
        "displays": [
            {
                "displayType": "query", -- 曝光类型
                "item": "3", -- 曝光对象 id
                "item_type": "sku_id", -- 曝光对象类型
                "order": 1 -- 曝光顺序
            },
            {
                "displayType": "promotion",
                "item": "6",
                "item_type": "sku_id",
                "order": 2
            },
            {
                "displayType": "promotion",
                "item": "9",
                "item_type": "sku_id",
                "order": 3
            },
            {
                "displayType": "recommend",
                "item": "6",
                "item_type": "sku_id",
                "order": 4
            },
            {
                "displayType": "query ",
                "item": "6",
                "item_type": "sku_id",
                "order": 5
            }
        ],
       "page": { -- 页面信息
                "during_time": 7648, -- 停留时间（毫秒）
                "page_item": "3", -- 页面对象 id
                "page_item_type": "sku_id", -- 页面对象类型
                "last_page_id": "login", -- 上页 id
                "page_id": "good_detail", -- 页面 ID
                "sourceType": "promotion" -- 页面来源类型
               },
        "err":{ -- 错误
               "error_code": "1234", -- 错误码
               "msg": "***********" -- 错误信息 },
         "ts": 1585744374423 -- 跳入时间
   }
   ```

2. 启动日志格式

   启动日志结构相对简单，主要包含公共信息，启动信息和错误信息。

   ```json
   {
       "common": {
           "ar": "370000",
           "ba": "Honor",
           "ch": "wandoujia",
           "md": "Honor 20s",
           "mid": "eQF5boERMJFOujcp",
           "os": "Android 11.0",
           "uid": "76",
           "vc": "v2.1.134"
         },
       "start": {
           "entry": "icon", -- 启动入口
           "loading_time": 18803, -- 启动加载时间
           "open_ad_id": 7, -- 开屏广告 id
           "open_ad_ms": 3449, -- 广告播放时间
           "open_ad_skip_ms": 1989 -- 用户跳过广告时间
       },
       "err":{ -- 错误
              "error_code": "1234", -- 错误码
              "msg": "***********" -- 错误信息
       },
       "ts": 1585744304000 -- 启动时间
   }
   ```

#### 3.2.3 埋点数据上报时机

埋点数据上报时机包括两种方式。

方式一，在离开该页面时，上传在这个页面发生的所有事情（页面、事件、曝光、错误等）。优点，批处理，减少了服务器接收数据压力。缺点，不是特别及时。

方式二，每个事件、动作、错误等，产生后，立即发送。优点，响应及时。缺点，对服务器接收数据压力比较大。

### 3.3 服务器和JDK准备

#### 3.3.1 服务器准备

使用虚拟机，参考vagrant+virtualbox

#### 3.3.2 阿里云服务器准备（可选）

#### 3.3.3 JDK 准备

[Centos7搭建java环境](https://www.wiz.cn/wapp/recent/?docGuid=989a8191-ed81-4220-81aa-e9198749f580&cmd=km%2C)

### 3.4 模拟数据

编写代码生成用户的行为日志。



## 4 数据采集模块

### 4.1 集群所有进程查看脚本

1. 在/home/bigdata/bin 目录下创建脚本 xcall.sh

   ```shell
   vim xcall.sh
   ```

2. 在脚本中编写如下内容

   ```shell
   #! /bin/bash 
   for i in hadoop102 hadoop103 hadoop104 
   do 
   	echo --------- $i ---------- 
   	ssh $i "$*" 
   done
   ```

3. 修改脚本的执行权限

   ```shell
   chmod 777 xcall.sh
   ```

4. 使用实例

   ```shell
   xcall.sh jps
   ```

### 4.2 Hadoop 安装

集群规划

| 主机名称 | IP              | 角色                                   | 系统名称                  |
| -------- | --------------- | -------------------------------------- | ------------------------- |
| node1    | 192.168.137.101 | namenode,datanode,nodemanager          | Centos release 7.4 x86_64 |
| node2    | 192.168.137.102 | secondarynamenode,datanode,nodemanager | Centos release 7.4 x86_64 |
| node3    | 192.168.137.103 | resourcemanager,datanode,nodemanager   | Centos release 7.4 x86_64 |

[2.7.2 集群安装](https://www.wiz.cn/wapp/recent/?docGuid=be9df7b5-2ef1-480e-ab8a-d6e7650ea08c&cmd=km%2C)

#### 4.2.1 项目经验之HDFS存储多目录

1. 生产环境服务器磁盘情况

   ![生产环境服务器磁盘情况](https://gitee.com/zhdoop/blogImg/raw/master/img/生产环境服务器磁盘情况.PNG)

2. 在hdfs-site.xml 文件中配置多目录，注意新挂载磁盘访问的访问权限问题。

   HDFS 的DataNode 节点保存数据的路径由<font color=red>dfs.datanode.data.dir</font>参数决定，其默认值为<font color=red>file://${hadoop.tmp.dir}/dfs/data</font>，若服务器有多个磁盘们必须对该参数进行修改。如服务器磁盘如上图所示，则该参数应修改为如下的值。

   ```xml
   <property>
       <name>dfs.datanode.data.dir</name>
       <value>file:///dfs/data1,file:///hd2/dfs/data2,file:///hd3/dfs/data3,file:///hd4/dfs/data4</value>
   </property>
   ```

   ***注意：因为每台服务器节点的磁盘情况不同，所以这个配置配完之后，不需要分发。***

#### 4.2.2 集群的数据均衡

##### 4.2.2.1 节点间数据均衡

开启数据均衡命令：

```
start-balancer.sh -threshold 10
```

对于参数10，代表的是集群中各个节点的磁盘利用率相差不超过10%，可根据实际情况进行调整。

停止数据均衡命令：

```shell
stop-balancer.sh
```

***注意：由于HDFS需要去哦启动单独的Rebalance Server 来执行 Rebalance 操作，所以尽量不要在NameNode 上执行start-balancer.sh，而是找一台比较空闲的机器***

##### 4.2.2.2 磁盘间数据均衡

1. 生成均衡计划（如果只有一块磁盘，不会生成计划）

   ```shell
   hdfs diskbalancer -plan node1
   ```

2. 执行均衡计划

   ```shell
   hdfs diskbalancer -execute node1.plan.json
   ```

3. 查看当前均衡任务的执行情况

   ```shell
   hdfs diskbalancer -query node1
   ```

4. 取消均衡任务

   ```shell
   hdfs diskbalancer -cancel node1.plan.json
   ```

#### 4.2.3 项目经验之支持LZO压缩配置

1. hadoop 本身并不支持lzo 压缩，故需要使用twitter 提供的hadoop-lzo 开源

组件。hadoop-lzo需依赖hadoop 和 lzo 进行编译，具体百度解决。

2. 将编译好的hadoop-lzo-0.4.20.jar 放入 hadoop-3.1.3/share/hadoop/common/

   ```shell
   pwd 
   /opt/module/node1/share/hadoop/common  
   ls hadoop-lzo-0.4.20.jar
   ```

3. 同步hadoop-lzo-0.4.20.jar 到 node2、node3

   ```shell
   xsync hadoop-lzo-0.4.20.jar
   ```

4. core.site.xml 增加配置支持LZO压缩

   ```xml
   <configuration> 
       <property> 
           <name>io.compression.codecs</name> 
           <value> 
               org.apache.hadoop.io.compress.GzipCodec, 		
               org.apache.hadoop.io.compress.DefaultCodec, 
               org.apache.hadoop.io.compress.BZip2Codec, 
               org.apache.hadoop.io.compress.SnappyCodec, 
               com.hadoop.compression.lzo.LzoCodec, 
               com.hadoop.compression.lzo.LzopCodec 
           </value> 
       </property> 
       <property> 
           <name>io.compression.codec.lzo.class</name> 
           <value>com.hadoop.compression.lzo.LzoCodec</value> 
       </property> 
   </configuration>
   ```

   5. 同步core-site.xml 到 node2、node3

      ```shell
      xsync core-site.xml
      ```

   6. 启动及查看集群

      ```shell
      sbin/start-dfs.sh
      sbin/start-yarn.sh
      ```

#### 4.2.4 项目经验之LZO 创建索引

1. 创建LZO文件的索引，LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO 压缩文件创建索引。如无索引，则LZO文件的切片只有一个。

   ```shell
   hadoop jar /path/to/your/hadoop-lzo.jar com.hadoop.compression.lzo.DistributedLzoIndexer big_file.lzo
   ```

2. 测试

   (1)将bigtable.lzo（200M）上传到集群的根目录

   ```shell
   hadoop fs -mkdir /input
   hadoop fs -put bigtable.lzo /input
   ```

   (2)执行wordcount 程序

   ```shell
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoText InputFormat /input /output1
   ```

   看到**number of splits:1**

   （3）对上传的LZO文件建索引

   ```shell
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20. jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo
   ```

   （4）在执行WordCount 程序

   ```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapredu ce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoText InputFormat /input /output2
   ```

​       看到**number of splits:2**

3. 注意：如果以上的任务，在运行过程中报如下异常：

   ```
   Container [pid=8468,containerID=container_1594198338753_0001_01_000002] is running 318740992B beyond the 'VIRTUAL' memory limit. Current usage: 111.5 MB of 1 GB physical memory used; 2.4 GB of 2.1 GB virtual memory used. Killing container. Dump of the process-tree for container_1594198338753_0001_01_000002 :
   ```

 

解决方法：在node1的/opt/module/hadoop-3-1.3/etc/hadoop/yarn-site.xml 文件中增如下的配置，然后分发到node2,nod3服务器上，并重新启动集群。

```xml
<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉， 默认是 true --> 
<property> 
    <name>yarn.nodemanager.pmem-check-enabled</name> 
    <value>false</value> 
</property> <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉， 默认是 true --> <property> 
    <name>yarn.nodemanager.vmem-check-enabled</name> 
    <value>false</value> 
</property>
```

#### 4.2.5 项目经验之基准测试

##### 4.2.5.1 测试HDFS写性能

测试内容：向 HDFS 集群写 10 个 128M 的文件

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-j obclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB

2020-04-16 13:41:24,724 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write 2020-04-16 13:41:24,724 INFO fs.TestDFSIO: Date & time: Thu Apr 16 13:41:24 CST 2020 
2020-04-16 13:41:24,724 INFO fs.TestDFSIO: Number of files: 10 
2020-04-16 13:41:24,725 INFO fs.TestDFSIO: Total MBytes processed: 1280 2020-04-16 13:41:24,725 INFO fs.TestDFSIO: Throughput mb/sec: 8.88 
2020-04-16 13:41:24,725 INFO fs.TestDFSIO: Average IO rate mb/sec: 8.96 
2020-04-16 13:41:24,725 INFO fs.TestDFSIO: IO rate std deviation: 0.87 2020-04-16 13:41:24,725 INFO fs.TestDFSIO: Test exec time sec: 67.61
```

***注意：nrFiles n为生成mapTask 的数量，生产环境一般可通过8088端口查看cpu核数，设置成cpu核数-1***

<table>
    <tr>
    	<th>参数</th>
        <th>说明</th>
    </tr>
     <tr>
    	<td>Number of files</td>
        <td>生成mapTask 数量，一般集群中CPU核数-1，我们测试的虚拟机就按照实际的物理内存-1分配即可</td>
    </tr>
    <tr>
    	<td>Total MBytes processed</td>
        <td>单个map处理的文件大小</td>
    </tr>
      <tr>
    	<td>Throughput mb/sec</td>
          <td>单个mapTask 的吞吐量；<br/><font color=green>计算方式：处理的总文件大小/每一个mapTask 写数据的时间累加></font><br/><font color=red>集体整体的吞吐量：生成mapTask数量*单个mapTask的吞吐量</font></td>
    </tr>
    <tr>
    	<td>Throughput mb/sec</td>
        <td>单个mapTask 的吞吐量；<br/><font color=green>计算方式：处理的总文件大小/每一个mapTask 写数据的时间累加></font><br/><font color=red>集体整体的吞吐量：生成mapTask数量*单个mapTask的吞吐量</font></td>
    </tr>
    <tr>
    	<td>Average IO rate mb/sec</td>
        <td><font color=green>每个 mapTask 处理文件大小/每一个 mapTask 写数据的时间 累加/生成 mapTask 数量</font></td>
    </tr>
     <tr>
    	<td>IO rate std deviation:</td>
        <td>方差、反映各个 mapTask 处理的差值，越小越均衡</td>
    </tr>
</table>

***注意：如果测试过程中，出现异常可以在yarn-site.xml中设置虚拟内存检测为false，分发配置并重启集群***

```xml
<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则 直接将其杀掉，默认是 true --> 
<property> 
    <name>yarn.nodemanager.vmem-check-enabled</name> 
    <value>false</value> 
</property>
```

##### 4.2.5.2 测试HDFS读性能

测试内容：读取 HDFS 集群 10 个 128M 的文件

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB


2020-04-16 13:43:38,857 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read 2020-04-16 13:43:38,858 INFO fs.TestDFSIO: Date & time: Thu Apr 16 13:43:38 CST 2020 
2020-04-16 13:43:38,859 INFO fs.TestDFSIO: Number of files: 10 
2020-04-16 13:43:38,859 INFO fs.TestDFSIO: Total MBytes processed: 1280 2020-04-16 13:43:38,859 INFO fs.TestDFSIO: Throughput mb/sec: 85.54 
2020-04-16 13:43:38,860 INFO fs.TestDFSIO: Average IO rate mb/sec: 100.21 2020-04-16 13:43:38,860 INFO fs.TestDFSIO: IO rate std deviation: 44.37 2020-04-16 13:43:38,860 INFO fs.TestDFSIO: Test exec time sec: 53.61
```

##### 4.2.5.3 删除测试生成数据

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapredu ce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
```

##### 4.2.5.4 使用Sort 程序评测MapReduce

1. 使用RandomWriter 来生产随机数，每个节点运行10个Map任务，每个Map产生大约1G大小的二进制随机数

   ```shell
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar randomwriter random-data
   ```

2. 执行Sort程序

   ```shell
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar sort random-data sorted-data
   ```

3. 验证数据是否真正排好序

   ```shell
   hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
   ```

#### 4.2.6 项目经验之Hadoop参数调优

##### 4.2.6.1 HDFS参数调优 hdfs-site.xml

```xml
The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes. NameNode 有一个工作线程池，用来处理不同 DataNode 的并发心跳以及客户端并发 的元数据操作。 对 于 大 集 群 或 者 有 大 量 客 户 端 的 集 群 来 说 ， 通 常 需 要 增 大 参 数 dfs.namenode.handler.count 的默认值 10。 
<property> 
    <name>dfs.namenode.handler.count</name> 
    <value>10</value> 
</property
```

$$
dfs.namenode.handler.count=20*log_e^{ClusterSize}
$$

比如集群规模为8时，此参数设置为41。

##### 4.2.6.2 YARN 参数调优 yarn-site.xml

（1）情景描述：总共7台机器，每天几亿条数据，数据源->Flume->Kafka->HDFS->Hive

面临问题：数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启了JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的很慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。

（2）解决方案：

内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop 单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。

（a) yarn.nodemanager.resource.memory-mb

表示该节点上 YARN 可使用的物理内存总量，默认是 8192（MB），注意，如果你的节点 

内存资源不够 8GB，则需要调减小这个值，而 YARN 不会智能的探测节点的物理内存总量。

（b) yarn.scheduler.maximum-allocation-mb

单个任务可申请的最多物理内存量，默认是 8192（MB）。

### 4.3 Zookeeper 安装

#### 4.3.1 安装ZK

集群规划

|           | 服务器node1 |   服务器node2   | 服务器node3 |
| --------- | ----------- | ---- | ----------- |
| Zookeeper |       Zookeeper      | Zookeeper | Zookeeper |

#### 4.3.2 ZK 集群启动停止脚本

1）在node1的/home/bigdata/bin 目录下创建脚本

```shell
vim zk.sh
```

在脚本中编写如下内容

```shell
#!/bin/bash 
case $1 in 
"start"){ 
	for i in node1 node2 node3 
	do 
		echo ---------- zookeeper $i 启动 ------------ 
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh start" 
	done 
};; 
"stop"){ 
	for i in node1 node2 node3  
	do 
		echo ---------- zookeeper $i 停止 ------------ 
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh stop" 
	done 
};; 
"status"){
	for i in node1 node2 node3  
	do 
		echo ---------- zookeeper $i 状态 ------------ 
		ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh status" 
	done 
};; 
esac
```

2）增加脚本执行权限

```shell
chmod u+x zk.sh
```

3) Zookeeper 集群启动脚本

```shell
zk.sh start
```

4) Zookeeper 集群停止脚本

```shell
zk.sh stop
```

### 4.4 Kafka 安装

#### 4.4.1 Kafka 集群安装

集群规划：
|           | 服务器node1 |   服务器node2   | 服务器node3 |
| --------- | ----------- | ---- | ----------- |
| Kafka |       Kafka      | Kafka | Kafka |

#### 4.4.2 Kafka 集群启动停止脚本

1）在/home/bigdata/bin 目录下创建脚本kf.sh

```shell
vim kf.sh
```

在脚本中填写如下内容

```shell
#! /bin/bash 
case $1 in 
"start"){ 
	for i in node1 node2 node3 
	do 
		echo " --------启动 $i Kafka-------" 
		ssh $i "/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties" 
	done 
};; 
"stop"){ 
	for i in node1 node2 node3
	do 
		echo " --------停止 $i Kafka-------" 
		ssh $i "/opt/module/kafka/bin/kafka-server-stop.sh stop" 
	done 
};; 
esac
```

2）增加脚本执行权限

```shell
chmod u+x kf.sh
```

3) kf集群启动脚本

```shell
kf.sh start
```

4) kf集群停止脚本

```shell
kf.sh stop
```

#### 4.4.3 Kafka 常用命令

1）查看Kafka Topic列表

```shell
bin/kafka-topic.sh node1:2181/kafka --list
```

2）创建Kafka Topic

进入到/opt/module/kafka/ 目录下创建日志主题

```shell
bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181/kafka --create --replication-factor 1 --partitions 1 --topic topic_log
```

3）删除Kafka Topic

```shell
bin/kafka-topics.sh --delete --zookeeper node1:2181,node2:2181,node3:2181/kafka --topic topic_log
```

4）Kafka 生产消息

```shell
bin/kafka-console-producer.sh  --broker-list node1:9092 --topic topic_log
```

5) Kafka 消费消息

```shell
bin/kafka-console-consumer.sh  --bootstrap-server node1:9092 --from-beginning --topic topic_log
```

--from-beginning：会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加 

该配置。

6）查看Kafka Topic详情

```shell
bin/kafka-topics.sh --zookeeper node1:2181/kafka  --describe --topic topic_log
```

#### 4.4.4 项目经验之Kafka压力测试

1）kafka压测

用Kafka官方自带的脚本，对Kafka进行压测。kafka压测时。可以查看到哪个地方出现了瓶颈（CPU，内存，网络IO）。一般都是网络IO达到瓶颈。

kafka-consumer-perf-test.sh

kafka-producer-perf-test.sh

2）Kafka Producer 压力测试

（1）在/opt/module/kafka/bin 目录下面有这两个文件，用此进行测试

```shell
bin/kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=node1:9092,node1:9092,node1:9092
```

说明：

record-size 是一条信息有多大，单位是字节

num-records 是总共发送了多条信息

throughput 是每条多少条信息，设成-1，表示不限流，可测试生产者最大的吞吐量

（2）Kafka 会打印下面的信息

```
100000 records sent, 95877.277085 records/sec (9.14 MB/sec), 187.68 ms avg latency, 424.00 ms max latency, 155 ms 50th, 411 ms 95th, 423 ms 99th, 424 ms 99.9th.
```

参数解析：本例中一共写入 10w 条消息，吞吐量为 **9.14 MB/sec**，每次写入的平均延迟为 187.68 毫秒，最大的延迟为 424.00 毫秒。

3）Kafka Consumer 压力测试

Consumer 的测试，如果这四个指标(IO、CPU、内存、网络)都不发生改变，考虑增加分区数来提升性能。

```shell
bin/kafka-consumer-perf-test.sh --broker-list node1:9092,node2:9092,node3:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1
```

参数说明：

--broker-list：kafka地址

--topic 指定topic的名称

--fetch-size 指定每次fetch 的数据的大小

--messages 总共要消费的消息个数

测试结果说明：

start.time, **end.time,** data.consumed.in.MB, **MB.sec,** data.consumed.in.nMsg**, nMsg.sec** 

2019-02-19 20:29:07:566, **2019-02-19 20:29:12:170,** 9.5368, **2.0714,** 100010, **21722.4153**

**开始测试时间，测试结束数据，共消费数据** 9.5368MB，吞吐量 **2.0714MB/s****，共消费** 

100010 条，平均每秒消费 **21722.4153** **条。**

#### 4.4.5 项目经验之Kafka 机器数量计算

Kafka 机器数量（经验公式）=2*（峰值生产速度*副本数/100）+1 

先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署 Kafka 的数量。 

比如我们的峰值生产速度是 50M/s。副本数为 2。 

Kafka 机器数量=2x（50x2/100）+ 1=3 台

#### 4.4.6 项目经验值Kafka分区数计算

1）创建一个只有 1 个分区的 topic 

2）测试这个 topic 的 producer 吞吐量和 consumer 吞吐量。 

3）假设他们的值分别是 Tp 和 Tc，单位可以是 MB/s。 

4）然后假设总的目标吞吐量是 Tt，那么分区数=Tt / min（Tp，Tc） 

例如：producer 吞吐量=20m/s；consumer 吞吐量=50m/s，期望吞吐量 100m/s； 

分区数=100 / 20 =5 分区 

分区数一般设置为：3-10 个

### 4.5 采集日志 Flume

#### 4.5.1 日志采集Flume 安装

集群规划

|                 | 服务器node1 | 服务器node2 | 服务器node3 |
| --------------- | ----------- | ----------- | ----------- |
| Flume(采集日志) | Flume       | Flume       |             |

#### 4.5.2 项目经验之Flume组件选型

1）source

(1) Taildir Source 相比 Exec Source、Spooling Directory Source 的优势

Taildir Source ：断点续传、多目录。Flume1.6 以前需要自己定义Source 记录每次读取文件位置，实现断点续传。

Exec Source ：可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据会丢失。

Spooling Directory Source ：监控目录，支持断点续传

(2) batchSize 大小如何设置

Event 1K左右时，500-1000 合适（默认为100）

2）Channel

  采用Kafka Channel，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。

注意在Flume1.7 以前，Kafka Channel 很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent 配置为true还是false，都会转为Flume Event。这样的话，造成的结果就是，会始终都把Flume的headers中信息混合着内容一起写入Kafka的消息中，这显然不是我们需要的，我们只需要内容。

#### 4.5.3 日志采集Flume配置

1）Flume 配置分析

![Flume采集](https://gitee.com/zhdoop/blogImg/raw/master/img/Flume采集.png)

Flume 直接读 log日志的数据，log日志的格式是app.yyyy-mm-dd.log。

Flume的具体配置如下：

（1）在/opt/module/flume/conf 目录下创建file-flume-kakfa.conf文件

```shell
vim file-flume-kafka.conf
```

在文件配置如下内容

```
#为各组件命名 
a1.sources = r1 
a1.channels = c1 
#描述 source 
a1.sources.r1.type = TAILDIR 
a1.sources.r1.filegroups = f1 
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.* a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json a1.sources.r1.interceptors = i1 
a1.sources.r1.interceptors.i1.type = com.bigdata.flume.interceptor.ETLInterceptor$Builder 
#描述 channel 
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel a1.channels.c1.kafka.bootstrap.servers = node1:9092,node2:9092 a1.channels.c1.kafka.topic = topic_log 
a1.channels.c1.parseAsFlumeEvent = false 
#绑定 source 和 channel 以及 sink 和 channel 的关系 
a1.sources.r1.channels = c1
```

***注意：com.bigdata.flume.interceptor.ETLInterceptor$Builder   是自定义的拦截器***

#### 4.5.4 Flume 拦截器

1）创建Maven 工程flume-interceptor

2）创建包名：com.bigdata.flume.interceptor

3)  在pom.xml 文件中添加如下配置

```xml
<dependencies> 
    <dependency> 
        <groupId>org.apache.flume</groupId> 
        <artifactId>flume-ng-core</artifactId> 
        <version>1.9.0</version> 
		<scope>provided</scope> 
	</dependency> 
    <dependency> 
        <groupId>com.alibaba</groupId> 
        <artifactId>fastjson</artifactId> 
        <version>1.2.62</version> 
    </dependency> 
</dependencies> 
<build> 
    <plugins> 
        <plugin> 
            <artifactId>maven-compiler-plugin</artifactId> 
            <version>2.3.2</version> 
            <configuration> 
                <source>1.8</source> 
                <target>1.8</target> 
            </configuration> 
        </plugin> 
        <plugin> 
            <artifactId>maven-assembly-plugin</artifactId> 
            <configuration> 
                <descriptorRefs> 
                    <descriptorRef>jar-with-dependencies</descriptorRef> 				 </descriptorRefs> 
            </configuration> 
            <executions> 
                <execution> 
                    <id>make-assembly</id> 
                    <phase>package</phase> 
                    <goals> 
                        <goal>single</goal> 
                    </goals> </execution> 
            </executions> 
        </plugin> 
    </plugins> 
</build>
```

***注意：scope 中 provided 的含义是编译时用该 jar 包。打包时时不用。因为集群上已经 
存在 flume 的 jar 包。只是本地编译时用一下***

4）在com.bigdata.flume.interceptor包下创建 JSONUtils 类

```java
package com.bigdata.flume.interceptor; 
import com.alibaba.fastjson.JSON; 
import com.alibaba.fastjson.JSONException; 
public class JSONUtils { 
    public static boolean isJSONValidate(String log){
        try {JSON.parse(log); return true; }
        catch (JSONException e){ return false; } 
    } 
}
```

5) 在com.bigdata.flume.interceptor 包下创建 LogInterceptor 类

```java
package com.bigdata.flume.interceptor; 
import com.alibaba.fastjson.JSON; 
import org.apache.flume.Context; 
import org.apache.flume.Event; 
import org.apache.flume.interceptor.Interceptor; 
import java.nio.charset.StandardCharsets; 
import java.util.Iterator; 
import java.util.List; 
public class ETLInterceptor implements Interceptor { 
    @Override 
    public void initialize() { 
    }
    @Override 
    public Event intercept(Event event) { 
        byte[] body = event.getBody(); 
        String log = new String(body, StandardCharsets.UTF_8); 
        if (JSONUtils.isJSONValidate(log)) { 
            return event; 
        } else { 
            return null; 
        } 
    }
    @Override 
    public List<Event> intercept(List<Event> list) { 
        Iterator<Event> iterator = list.iterator(); 
        while (iterator.hasNext()){ 
            Event next = iterator.next(); 
            if(intercept(next)==null){
                iterator.remove(); 
            } 
        }return list; 
    }
    
    public static class Builder implements Interceptor.Builder{ 
        @Override 
        public Interceptor build() {
            return new ETLInterceptor(); 
        }
        @Override 
        public void configure(Context context) { } 
    }
    
    @Override public void close() { } 
}
```

6）打包

7）需要先将打好包放入到node1 的/opt/module/flume/lib 文件夹下面

```shell
ls | grep interceptor flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar
```

8） 分发Flume到node2、node3

```shell
xsync flume/
```

9) 分别在node1、node3 上启动Flume

```shell
bin/flume-ng agent --name a1 --conf-file conf/file-flume-kafka.conf &
```

#### 4.5.5 日志采集Flume启动停止脚本

1) 在/home/bingdata/bin 目录下创建脚本f1.sh

```shell
vim f1.sh
```

在脚本中填写如下的内容

```shell
#! /bin/bash 
case $1 in 
"start"){ 
	for i in node1 node2 
	do
		echo " --------启动 $i 采集 flume-------" 
		ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log1.txt 2>&1 &"
		done 
};; 
"stop"){
	for i in node1 node2 
	do 
		echo " --------停止 $i 采集 flume-------" 
		ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk '{print \$2}' | xargs -n1 kill -9 " 
		done 
};; 
esac
```

说明 1：nohup，该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。nohup 

就是不挂起的意思，不挂断地运行命令。 

说明 2：awk 默认分隔符为空格 

说明 3：xargs 表示取出前面命令运行的结果，作为后面命令的输入参数。

2）增加脚本的权限

```shell
chmod u+x f1.sh
```

3）f1 集群启动脚本

```shell
f1.sh start
```

4）f1 集群停止脚本

```shell
f1.sh stop
```

### 4.6 消费Kafka 数据 Flume

|                  | 服务器node1 | 服务器node2 | 服务器node3 |
| ---------------- | ----------- | ----------- | ----------- |
| Flume(消费Kafka) |             |             | Flume       |

#### 4.6.1 项目经验之Flume组件选型

1）FileChannel 和 MemoryChannel 区别

MemoryChannel 传输数据速度更快，但因为数据保存在 JVM 的堆内存中，Agent 进程 

挂掉会导致数据丢失，适用于对数据质量要求不高的需求。 

FileChannel 传输速度相对于 Memory 慢，但数据安全保障高，Agent 进程挂掉也可以从 

失败中恢复数据

选型： 

金融类公司、对钱要求非常准确的公司通常会选择 FileChannel 

传输的是普通日志信息（京东内部一天丢 100 万-200 万条，这是非常正常的），通常 

选择 MemoryChannel。

2） FileChannel 优化

通过配置 dataDirs 指向多个路径，每个路径对应不同的硬盘，增大 Flume 吞吐量。

checkpointDir 和 backupCheckpointDir 也尽量配置在不同硬盘对应的目录中，保证 

checkpoint 坏掉后，可以快速使用 backupCheckpointDir 恢复数据

3）Sink：HDFS Sink

（1）HDFS 存入大量小文件，有什么影响？

**元数据层面：**每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所属 

组，权限，创建时间等，这些信息都保存在 Namenode 内存中。所以小文件过多，会占用 

Namenode 服务器大量内存，影响 Namenode 性能和使用寿命

**计算层面：**默认情况下 MR 会对每个小文件启用一个 Map 任务计算，非常影响计算性 

能。同时也影响磁盘寻址时间。

（2）HDFS 小文件处理 

官方默认的这三个参数配置写入 HDFS 后会产生小文件，hdfs.rollInterval、hdfs.rollSize、 

hdfs.rollCount 

基于以上 hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0 几个参数综

合作用，效果如下： 

（1）文件在达到 128M 时会滚动生成新文件 

（2）文件创建超 3600 秒时会滚动生成新文件

#### 4.6.2 Flume 拦截器

由于 Flume 默认会用 Linux 系统时间，作为输出到 HDFS 路径的时间。如果数据是 23:59 

分产生的。Flume 消费 Kafka 里面的数据时，有可能已经是第二天了，那么这部门数据会被 

发往第二天的 HDFS 路径。我们希望的是根据日志里面的实际时间，发往 HDFS 的路径， 

所以下面拦截器作用是获取日志中的实际时间。

解决的思路：拦截 json 日志，通过 fastjson 框架解析 json，获取实际时间 ts。将获取的 

ts 时间写入拦截器 header 头，header 的 key 必须是 timestamp，因为 Flume 框架会根据这个 

key 的值识别为时间，写入到 HDFS。 

1）在 com.bigdata.flume.interceptor 包下创建 TimeStampInterceptor 类 

```java
package com.bigdata.flume.interceptor; 
import com.alibaba.fastjson.JSONObject; 
import org.apache.flume.Context; 
import org.apache.flume.Event; 
import org.apache.flume.interceptor.Interceptor; 
import java.nio.charset.StandardCharsets; 
import java.util.ArrayList; 
import java.util.List; 
import java.util.Map; 
public class TimeStampInterceptor implements Interceptor { 
    private ArrayList<Event> events = new ArrayList<>(); 
    @Override 
    public void initialize() { }
    @Override 
    public Event intercept(Event event) { 
        Map<String, String> headers = event.getHeaders(); 
        String log = new String(event.getBody(), StandardCharsets.UTF_8); 
        JSONObject jsonObject = JSONObject.parseObject(log); 
        String ts = jsonObject.getString("ts"); 
        headers.put("timestamp", ts); 
        return event;
    }
    @Override 
    public List<Event> intercept(List<Event> list) { 
        events.clear(); 
        for (Event event : list) { 
            events.add(intercept(event)); 
        }
        return events; 
    }
    
    @Override 
    public void close() { }
    
    public static class Builder implements Interceptor.Builder { 
        @Override 
        public Interceptor build() { 
            return new TimeStampInterceptor(); 
        }
        @Override 
        public void configure(Context context) { } 
    } 
}
```

2）重新打包

3）需要先将打好包放入到node1的/opt/module/flume/lib 文件夹下面。

```shell
ls | grep interceptor flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar
```

4）分发Flume 到 node2、node3

```shell
xsync flume/
```

#### 4.6.3 日志消费Flume配置

1）Flume配置分析

![Flume消费](https://gitee.com/zhdoop/blogImg/raw/master/img/日志消费Flume.png)

2）Flume 的具体配置如下：

（1）在node3的/opt/module/flume/conf 目录下创建 kafka-flume-hdfs.conf 文件 

```shell
vim kafka-flume-hdfs.conf
```

在文件配置如下内容

```
## 组件 
a1.sources=r1 
a1.channels=c1 
a1.sinks=k1 
## source1 
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource 
a1.sources.r1.batchSize = 5000 
a1.sources.r1.batchDurationMillis = 2000 
a1.sources.r1.kafka.bootstrap.servers = node1:9092,node2:9092,node3:9092 a1.sources.r1.kafka.topics=topic_log 
a1.sources.r1.interceptors = i1 
a1.sources.r1.interceptors.i1.type = com.bigdata.flume.interceptor.TimeStampInterceptor$Builder 
## channel1 
a1.channels.c1.type = file 
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior1 a1.channels.c1.dataDirs = /opt/module/flume/data/behavior1/ 
a1.channels.c1.maxFileSize = 2146435071 
a1.channels.c1.capacity = 1000000 
a1.channels.c1.keep-alive = 6 

## sink1 
a1.sinks.k1.type = hdfs 
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log- 
a1.sinks.k1.hdfs.round = false 
a1.sinks.k1.hdfs.rollInterval = 10 
a1.sinks.k1.hdfs.rollSize = 134217728 
a1.sinks.k1.hdfs.rollCount = 0 ## 控制输出文件是原生文件。 
a1.sinks.k1.hdfs.fileType = CompressedStream 
a1.sinks.k1.hdfs.codeC = lzop 

## 拼装 
a1.sources.r1.channels = c1 
a1.sinks.k1.channel= c1
```

#### 4.6.4 日志消费Flume启动停止脚本

1）在/home/atguigu/bin 目录下创建脚本 f2.sh

```shell
vim f2.sh
```

在脚本中填写如下内容

```shell
#! /bin/bash 
case $1 in "start"){
	for i in node3 
	do 
		echo " --------启动 $i 消费 flume-------" 
		ssh $i "nohup /opt/module/flume/bin/flume-ng agent --conf-file /opt/module/flume/conf/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >/opt/module/flume/log2.txt 2>&1 &"
	done 
};; 
"stop"){
	for i in node3 
	do 
		echo " --------停止 $i 消费 flume-------" 
		ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '{print \$2}' | xargs -n1 kill" 
	done 
};; 
esac
```

2）增加脚本执行权限

```shell
chmod u+x f2.sh
```

3）f2 集群启动脚本

```shell
f2.sh start
```

4）f2 集群停止脚本

```shell
f2.sh stop
```

#### 4.6.5 项目经验之Flume内存优化

1）问题描述：如果启动消费 Flume 抛出如下异常

<font color=red>ERROR hdfs.HDFSEventSink: process failed java.lang.OutOfMemoryError: GC overhead limit exceeded</font>

2）解决方案步骤：

（1）在 node1 服务器的/opt/module/flume/conf/flume-env.sh 文件中增加如下配置

```
export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote"
```

（2）同步配置到 node2、node3服务器

```shell
xsync flume-env.sh
```

3）Flume 内存参数设置及优化

JVM heap 一般设置为 4G 或更高 

-Xmx 与-Xms 最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导 

致频繁 fullgc。 

-Xms 表示 JVM Heap(堆内存)最小尺寸，初始分配；-Xmx 表示 JVM Heap(堆内存)最 

大允许的尺寸，按需分配。如果不设置一致，容易在初始化时，由于内存不够，频繁触发 fullgc。

### 4.7 采集通道启动/停止脚本

#### 4.7.1 数据通道测试

根据需求分别生成 2020-06-14 和 2020-06-15 日期的数据

在这个期间，不断观察 Hadoop 的 HDFS 路径上是否有数据

#### 4.7.2 采集通道启动/停止脚本

1）在/home/bigdata/bin 目录下创建脚本 cluster.sh

```shell
vim cluster.sh
```

在脚本中填写如下内容

```shell
#!/bin/bash
case $1 in
"start"){ 
   echo ================== 启动 集群 ================== 
   #启动 Zookeeper 集群 
   zk.sh start 
   #启动 Hadoop 集群 
   hdp.sh start 
   #启动 Kafka 采集集群 
   kf.sh start 
   #启动 Flume 采集集群 
   f1.sh start 
   #启动 Flume 消费集群 
   f2.sh start 
};;
"stop"){
   echo ================== 停止 集群 ================== 
   #停止 Flume 消费集群 
   f2.sh stop 
   #停止 Flume 采集集群 
   f1.sh stop 
   #停止 Kafka 采集集群 
   kf.sh stop 
   #停止 Hadoop 集群 
   hdp.sh stop
   #停止 Zookeeper 集群 
   zk.sh stop 
};; 
esac
```

2）增加脚本执行权限

```shell
chmod u+x cluster.sh
```

3）cluster 集群启动脚本

```shell
cluster.sh start
```

4）cluster 集群停止脚本

```shell
cluster.sh stop
```

