# 电商数据仓库系统

## 1 数仓分层

### 1.1 为什么要分层

![数仓分层](https://gitee.com/zhdoop/blogImg/raw/master/img/数仓分层.PNG)

![数据仓库为什么分层](https://gitee.com/zhdoop/blogImg/raw/master/img/数据仓库为什么分层.PNG)

### 1.2 数据集市与数据仓库的概念

**数据集市 **是一种微型的数据仓库，它通常有更少的数据，更少的主题区域，以及更少的历史数据，因此是部门级别的，一般只能为某个局部范围内的管理人员服务。

**数据仓库** 是企业级别的，能为整个企业各个部门的运行提供决策支持手段。

![数据仓库与数据集市](https://gitee.com/zhdoop/blogImg/raw/master/img/数据仓库与数据集市.PNG)

### 1.3 数仓命名规范

#### 1.3.1 表命名

* ODS层命名为ods_表名
* DWD层命名为dwd_dim/fact_表名
* DWS层命名为dws_表名
* DWT层命名为dwt_表名
* ADS层命名为ads_表名
* 临时表命名为xxx_tmp
* 用户行为表，以log为后缀

#### 1.3.2 脚本命名

* 数据源\_to\_目标\_db/log.sh
* 用户行为脚本以log为后缀；业务数据脚本以db为后缀

#### 1.3.3 表字段类型

* 数量类型为bigint
* 金额类型为decimal(6,2)，表示：16位有效数字，其中小数部分2位
* 字符串（名字，描述信息等）类型为string
* 主键外键类型是string
* 时间戳类型为bigint

## 2 数仓理论

### 2.1 范式理论

#### 2.1.1 范式概念

1）定义

范式可以理解为设计一张数据表的结构，符合的标准级别、规范和要求。

2）优点

采用范式，可以降低数据的冗余性

为什么要降低数据冗余性？

（1）十几年前，磁盘很贵，为了减少磁盘存储。 

（2）十几年前，磁盘很贵，为了减少磁盘存储。 

（3）一次修改，需要修改多个表，很难保证数据一致性

3）缺点

范式的缺点是获取数据时，需要通过 Join 拼接出最后的数据

4）分类

目前业界范式有：第一范式(1NF)、第二范式(2NF)、第三范式(3NF)、巴斯-科德范式 

(BCNF)、第四范式(4NF)、第五范式(5NF)。

#### 2.1.2 函数依赖

![函数依赖](https://gitee.com/zhdoop/blogImg/raw/master/img/函数依赖1.PNG)

#### 2.1.3 三范式区分

##### 2.1.3.1 第一范式

![第一范式](https://gitee.com/zhdoop/blogImg/raw/master/img/第一范式.PNG)

##### 2.1.3.2 第二范式

![第二范式](https://gitee.com/zhdoop/blogImg/raw/master/img/第二范式.PNG)

##### 2.1.3.3 第三范式

![第三范式](https://gitee.com/zhdoop/blogImg/raw/master/img/第三范式.PNG)

### 2.2 关系建模与维度建模

​	当今的数据处理大致可以分成两大类：联机事务处理 OLTP（on-line transaction processing）、联机分析处理 OLAP（On-Line Analytical Processing）。OLTP 是传统的关系 型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。OLAP 是数据仓库 系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。二者的主要区别对比如下表所示。

| 对比属性 | OLTP                       | OLAP                       |
| -------- | -------------------------- | -------------------------- |
| 读特性   | 每次查询只返回少量记录     | 对大量记录进行汇总         |
| 写特性   | 随机、低延迟写入用户的输入 | 批量导入                   |
| 使用场景 | 用户，JavaEE项目           | 内部分析师，为决策提供支持 |
| 数据特征 | 最新数据状态               | 随时间变化的历史状态       |
| 数据规模 | GB                         | TB 到 PB                   |

#### 2.2.1 关系建模

![关系建模](https://gitee.com/zhdoop/blogImg/raw/master/img/关系建模.PNG)

关系模型如图所示，严格遵循第三范式（3NF），从图中看出，较为松散、零碎，物理表数量多，而数据冗余程度低。由于数据分布于众多的表中，这些数据可以更改为为更加灵活地被应用，功能较强。关系模型主要应用于OLTP系统中，为了保证数据的一致性以及避免冗余，所以大部分业务系统的表都是遵循第三范式的。

#### 2.2.2 维度建模

![维度建模](https://gitee.com/zhdoop/blogImg/raw/master/img/维度建模.PNG)

维度建模如图所示，主要应用于OLAP系统中，通常以某一个事实表为 中心进行表的组织，主要面向业务，特征是可能存在数据的冗余，但是能方便得到数据。

关系模型虽然冗余少，但是在大规模数据，跨表分析统计查询过程中，会造成多表关联，这会大大降低执行效率。所以通常我们采用维度模型建模，把相关各种表整理成两种：事实表和维度表。

在维度建模的基础上又分为三种模型：星型模型、雪花模型、星座模型。

![星型模型](https://gitee.com/zhdoop/blogImg/raw/master/img/星型模型.PNG)

![雪花模型](https://gitee.com/zhdoop/blogImg/raw/master/img/雪花模型.PNG)

![星座模型](https://gitee.com/zhdoop/blogImg/raw/master/img/星座模型.PNG)

4.模型的选择

首先就是星座不星座这个只跟数据和需求有关系，跟设计没关系，不用选择。

星型还是雪花，取决于性能优先，还是灵活更优先。

目前实际企业开发中，不会绝对选择一种，根据情况灵活组合，甚至并存（一层维度和多层维度都保存）。但是整体来看，更倾向于维度更少的星型。尤其是Hadoop体系，减少Join就是减少Shuffle，性能差距很大。（关系型数据库可以依靠强大的主键索引）

### 2.3 维度表和事实表（重点）

#### 2.3.1 维度表

维度表：一般是对事实的描述信息。每一张维度表对应现实世界中的一个对象或者概念。例如：用户、商品、日期、地区等。

**维表的特征**

* 维度的范围很宽（具有多个属性、列比较多）
* 跟事实表相比，行数相对较少：通常<10万条
* 内容相对固定：编码表

时间维度表：

![时间维度表](https://gitee.com/zhdoop/blogImg/raw/master/img/时间维度表.PNG)

#### 2.3.2 事实表

事实表中的每行数据代表一个业务事件（下单、支付、退款、评价等）。“事实”这个术语表示的是业务事件的度量值（可统计次数、个数、金额等），例如，2020年5月21 日，宋宋老师在京东发了 250 块钱买了一瓶海狗人参丸。维度表：时间、用户、商品、商家。 事实表：250 块钱、一瓶

每一个事实表的行包括：具有可加性的数值型的度量值、与维表相连接的外键、通常具 有两个和两个以上的外键、外键之间表示维表之间多对多的关系。

**事实表的特征**

* 非常的大
* 内容相对的窄：列数较少（主要是外键Id 和 度量值）
* 经常发生变化，每天会新增加很多

1） 事实型事实表

​	以每个事务或事件为单位，例如一个销售订单记录，一笔支付记录等，作为事实表里的一行数据。一旦事务被提交，事实表数据被插入，数据就不会进行更改，其更新方式是增量更新。

2）周期型快照事实表

​	周期型快照事实表中不会保留所有数据，只保留固定时间间隔的数据，例如每天或者每月的销售额，或每月的账号余额等。

​	例如购物车，有加减商品，随时都有可能变化，但是我们更关心每天结束时这里面有多少商品，方便我们后期统计分析。

3）累计型快照事实表

​	累计快照型事实表用于跟踪业务事实的变化。例如，数据仓库中可能需要累积或者存储订单从下单开始，到订单商品被打包、运输、和签收的各个业务阶段的时间点数据来跟踪订单声明周期的进展情况。当这个业务过程进行时，事实表的记录也要不断更新。

| 订单id | 用户id | 下单时间 | 打包时间 | 发货时间 | 签收时间 | 订单金额 |
| ------ | ------ | -------- | -------- | -------- | -------- | -------- |
|        |        | 3-8      | 3-8      | 3-9      | 3-10     |          |

### 2.4 数据仓库建模

#### 2.4.1 ODS层

1） HDFS 用户行为数据

![HDFS用户行为数据](https://gitee.com/zhdoop/blogImg/raw/master/img/HDFS用户行为数据.PNG)

2）HDFS 业务数据

![HDFS业务数据](https://gitee.com/zhdoop/blogImg/raw/master/img/HDFS业务数据.PNG)

3）针对HDFS上的用户行为数据和业务数据，我们如何规划和处理？

（1）保持数据原貌不做修改，起到备份数据的作用。

（2）数据采用压缩，减少磁盘存储空间（例如：原始数据100G，可以压缩到10G左右）

（3）创建分区表，防止后续的全表扫描

#### 2.4.2 DWD 层

DWD层构建维度模型，一般采用星型模型，呈现的状态一般为星座模型。

维度建模一般按照以下四个步骤：

<font color=red>选择业务过程-->声明力度-->确认维度-->确认事实</font>

##### 2.4.2.1 选择业务过程

在业务系统中，挑选我们感兴趣的业务线，比如下单业务，支付业务，退款业务，物流业务，一条业务线对应一张事实表。

如果是中小公司，尽量把所有业务过程都选择。

如果是大公司（1000 多张表），选择和需求相关的业务线。

##### 2.4.2.2 声明粒度

数据粒度指数据仓库的数据中保存数据的细化程度和综合程度的级别。

声明粒度意味着精确定义事实表中的一行数据表示什么，应该尽可能选择最小粒度，以此来应付各种各样的需求。

**典型的粒度声明如下**：

订单当中每个商品项作为下单事实表中的一行，粒度为每次。

每周的订单次数作为一行，粒度为每周。

每月的订单次数作为一行，粒度为每月。

如果在DWD层粒度就是每周或者每月，那么后续就没有办法统计细粒度的指标了。所以建议采用最小粒度。

##### 2.4.2.3 确定维度

维度的主要作用是描述业务是事实，主要表示是"谁，何处，何时"等信息。

确定维度原则是： 后续需求中是否要分析相关维度的指标。例如，需要统计什么时间下单多，哪个地区下的订单多，哪个用户下的订单多。需要确定的维度就包括：时间维度、地区维度、用户维度。

维度表：需要根据维度建模中星型模型原则进行维度退化。

##### 2.4.2.4 确定事实

此处的“事实”一词，指的是业务中的度量值（次数、个数、件数、金额，可以进行累加），例如订单金额、下单次数等。

在DWD层，以业务过程为建模驱动，基于每个具体业务过程的特点，构建最细粒度的明细层事实表。事实表可做适当的宽表化处理。

事实表和维度表的关联比较灵活，但是为了应对更复杂的业务需求，可以将能关联上的表尽量关联上。如何判断是否能够关联上呢？在业务表关系图中，只要两张表能通过中间表能够关联上吗，就说明能关联上。

![电商事实表关联](https://gitee.com/zhdoop/blogImg/raw/master/img/电商事实表关联.PNG)

至此，数据仓库的维度已经完毕，DWD层是以业务过程为驱动。

DWS层、DWT层和ADS层都是以需求为驱动，和维度建模已经没有关系了。

DWS层和DWT层都是建宽表，按照主题去建表。主题相当于观察问题的角度。对应着维度表。

![事实表图](https://gitee.com/zhdoop/blogImg/raw/master/img/事实表图1.PNG)

#### 2.4.3 DWS层

DWS层统计各个主题对象的当天行为，服务于DWT层的主要宽表

（1）问题引出：两个需求，统计每个省份订单的个数、统计每个省份订单的总金额

（2）处理方法：都是将省份表和订单表进行join，group by 省份，然后计算。相当于类似的需求重复计算了两次。

那怎么设计能避免重复计算呢？

地区宽表的字段设计为：下单次数、下单金额、支付次数、支付金额等。只需要和每个事实表一次join。

（3）总结：

需要建哪些表：以维度为基准，去关联对应的多个事实表。

宽表里面的字段，是站在不同维度的角度去看事实表，重点关注事实表聚合后的度量值。

![dws用户维度示例](https://gitee.com/zhdoop/blogImg/raw/master/img/dws表.PNG)

#### 2.4.4 DWT层

DWT层统计各个主题对象的累积行为。

（1）需要建哪些表：和DWS层一样。以维度为基准，去关联对应多个事实表。

（2）宽表里面的字段：我们站在维度表的角度去看事实表，重点关注事实表度量值的累积值、事实表行为的首次和末次时间。例如，订单事实表的度量值是下单次数、下单金额。订单事实表的行为是下单。

我们站 在用户维度表的角度去看订单事实表，重点关注订单事实表至今的累积下单次数、累积下单金额和某时间段内的累积次数、累积金额，以及关注下单行为的首次时间和末次时间。

​	![DWT层用户表示例](https://gitee.com/zhdoop/blogImg/raw/master/img/DWT层用户行为示例.PNG)

#### 2.4.5 ADS层

对电商系统各大主题指标进行分析。

## 3 数仓搭建-ODS层

1）保持数据原貌不做任何修改，起到备份数据的作用

2）数据采用LZO压缩，减少 磁盘存储空间。100G数据可以压缩到10G以内

3）创建分区表，防止后续的全表扫描，在企业开发中大量使用分区表。

4）创建外部表。在企业开发中，除了自己用的临时表，创建内部表外，绝大多数场景都是创建外部表。

### 3.1 Hive环境准备

#### 3.1.1 Hive引擎简介

Hive 引擎包括：默认MR、tez、spark

Hive on Spark：Hive 既作为存储元数据又负责SQL解析优化，语法是HQL语法，执行引擎变成了 Spark，Spark负责采用RDD执行。

Spark on Hive：Hive 只作为元数据存储，Spark负责SQL解析优化，语法是Spark SQL语法，Spark负责采用RDD执行。

#### 3.1.2 Hive on Spark 配置

1）兼容性说明

注意：官网下载的Hive3.1.2 和 Spark3.0.0默认是不兼容的。因为Hive3.1.2 支持的Spark版本是2.4.5，所以我们需要重新编译Hive3.1.2 版本。

**编译步骤**

官网下载Hive3.1.2 源码，修改pom文件中引用的Spark版本为3.0.0，如果编译通过，直接打包获取jar包。如果报错，修改相关方法，直到不报错，打包获取jar包。

2）在Hive所在节点部署Spark

如果之前已经部署了Spark，则该步骤可以跳过，但要检查SPARK_HOME的环境变量配置是否正确。

（1）Spark官网下载jar包地址

http://spark.apache.org/downloads.html

（2）上传并解压spark-3.0.0-bin-hadoop3.2.tgz

```shell
$ tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/ 
$ mv /opt/module/spark-3.0.0-bin-hadoop3.2 /opt/module/spark
```

（3）配置SPARK_HOME环境变量

```shell
sudo vim /etc/profile.d/my_env.sh
```

添加如下内容

```
# SPARK_HOME 
export SPARK_HOME=/opt/module/spark 
export PATH=$PATH:$SPARK_HOME/bin
```

source 使其生效

```shell
source /etc/profile.d/my_env.sh
```

（4）新建spark配置文件

```shell
vim /opt/module/hive/conf/spark-defaults.conf
```

添加如下内容（在执行任务时，会根据如下参数执行）

```shell
spark.master yarn 
spark.eventLog.enabled true 
spark.eventLog.dir hdfs://node1:8020/spark-history 
spark.executor.memory 1g 
spark.driver.memory 1g
```

（5）在HDFS创建如下路径，用于存储历史日志

```shell
hadoop fs -mkdir /spark-history
```

3） 向HDFS上上传Spark纯净版jar包

说明1：由于Spark3.0.0 非纯净版，默认支持的是hive2.3.7版本，直接使用会和安装的Hive3.1.2 出现兼容性问题。所以采用Spark纯净版jar包，不包含hadoop和hive相关依赖，避免冲突。

说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到。

（1）上传并解压spark-3.0.0-bin-without-hadoop,tgz

```shell
tar -zxvf /opt/software/spark-3.0.0-bin-without-hadoop.tgz
```

(2)上传Spark纯净版jar包到HDFS

```shell
hadoop fs -mkdir /spark-jars
hadoop fs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars
```

4）修改hive-site.xml 文件

```shell
vim /opt/module/hive/conf/hive-site.xml
```

添加如下内容

```xml
<!--Spark 依赖位置（注意：端口号 8020 必须和 namenode 的端口号一致）--> 
<property> 
    <name>spark.yarn.jars</name> 
    <value>hdfs://node1:8020/spark-jars/*</value> 
</property> 
!--Hive 执行引擎--> <property> 
    <name>hive.execution.engine</name> 
    <value>spark</value> 
</property>
<!--Hive 和 Spark 连接超时时间--> 
<property>
    <name>hive.spark.client.connect.timeout</name> 
    <value>10000ms</value> 
</property>
```

注意：hive.spark.client.connect.timeout 的默认值是 1000ms，如果执行 hive 的 insert 语句 时，抛如下异常，可以调大该参数到 10000ms

```shell
FAILED: SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create Spark client for Spark session d9e0224c-3d14-4bf4-95bc-ee3ec56df48e
```

#### 3.1.3 Hive on Spark 测试

（1）启动hive客户端

```shell
bin/hive
```

（2）创建一张测试表

```sql
hive (default)> create table student(id int, name string);
```

（3）通过 insert 测试效果

```sql
hive (default)> insert into table student values(1,'abc');
```

若结果如下，则说明配置成功

![测试结果](https://gitee.com/zhdoop/blogImg/raw/master/img/Hive_on_Spark测试结果1.PNG)

#### 3.1.4 Yarn 容量调度器并发度问题演示

Yarn 默认调度器为 Capacity Scheduler（容量调度器），且默认只有一个队列——default。 如果队列中执行第一个任务资源不够，就不会再执行第二个任务，一直等到第一个任务执行 完毕。

（1）启动1个hive客户端，执行以下插入数据的SQL语句

```sql
hive (default)> insert into table student values(1,'abc');
```

执行该语句，hive 会初始化一个 Spark Session，用以执行 hive on spark 任务。由于未指 定队列，故该 Spark Session 默认占用使用的就是 default 队列，且会一直占用该队列，直到 退出 hive 客户端。

可以访问ResourceManager的web 页面查看相关信息。

![webui](https://gitee.com/zhdoop/blogImg/raw/master/img/Hive_on_Sparkwebui.PNG)

（2）在hive端开启的情况下，提交一个MR

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples -3.1.3.jar pi 1 1
```

MR 任务同样未指定队列，所以其默认也提交到了 default 队列，由于容量调度器单个队列的并行度为 1。故后提交的 MR 任务会一直等待，不能开始执行。 

任务提交界面如下：

![测试结果](https://gitee.com/zhdoop/blogImg/raw/master/img/Yarn容量调度一个队列测试结果1.PNG)

ResourceManager 的 web 页面如下：

![webUi](https://gitee.com/zhdoop/blogImg/raw/master/img/Yarn容量调度一个队列webUI.PNG)

（3）容量调度器 default 队列中，同一时间只有一个任务执行，并发度低，如何解决呢？ 

方案一：增加 ApplicationMaster 资源比例，进而提高运行 app 数量。 

方案二：创建多队列，比如增加一个 hive 队列。

#### 3.1.5 增加ApplicationMaster 资源比例

针对容量调度器并发度低的问题，考虑调整yarn.scheduler.capacity.maximum-am-resource-percent 该参数。默认值是 0.1，表示集群上 AM 最多可使用的资源比例，目的为限制过多的 app 数量。

（1）在node1 的/opt/module/hadoop-3.1.3/etc/Hadoop/capacity-scheduler.xml 文件中 修改如下参数值。

```xml
vim capacity-scheduler.xml
<property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>0.5</value>
    <description> 集群中用于运行应用程序 ApplicationMaster 的资源比例上限， 该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型， 默认是 0.1，表示 10%。所有队列的 ApplicationMaster 资源比例上限可通过参数 yarn.scheduler.capacity.maximum-am-resource-percent 设置，而单个队 列 可 通 过 参 数yarn.scheduler.capacity.
        {queue-path}.maximum-am-resource-percent 设置适合自己的值。
    </description>
</property>
```

（3） 分发capacity-scheduler.xml 配置文件

```shell
xsync capacity-scheduler.xml
```

（3）关闭正在运行的任务，重新启动yarn集群

```shell
sbin/stop-yarn.sh
sbin/start-yarn.sh
```

#### 3.1.6 增加Yarn容量调度器队列

方案二：创建多队列，也可以增加容量调度器的并发度。

在企业里面如何配置多队列：

* 按照计算引擎创建队列 hive、spark、flink

* 按照业务创建队列：下单、支付、点赞、评论、收藏（用户、活动、优惠相关）

有什么好处？

（1）假如公司来了一个菜鸟，写了一个递归死循环，公司集群资源耗尽，大数据全部瘫痪。

（2）**解耦。**假如 11.11 数据量非常大，任务非常多，如果所有任务都参与运行，一定执行不完，怎 

么办？

（3）**可以支持降级运行。**

​	下单 √

​			支付√

​					点赞 X 

**1）增加容量调度器队列 **

（1）修改容量调度器配置文件文件

默认 Yarn 的配置下，容量调度器只有一条 default 队列。在 capacity-scheduler.xml 中可以配置多条队列，**修改**以下属性，增加 hive 队列。

```shell
<property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default,hive</value>
    <description> 再增加一个 hive 队列 </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>50</value>
    <description>default 队列的容量为 50% </description>
</property>
```

同时为新加队列**添加**必要属性：

```xml
<property>
    <name>yarn.scheduler.capacity.root.hive.capacity</name>
    <value>50</value>
    <description> hive 队列的容量为 50% </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
    <value>1</value>
    <description> 一个用户最多能够获取该队列资源容量的比例，取值 0-1 </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
    <value>80</value>
    <description> hive 队列的最大容量（自己队列资源不够，可以使用其他队列资源上限） </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.state</name>
    <value>RUNNING</value>
    <description> 开启 hive 队列运行，不设置队列不能使用 </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
    <value>*</value>
    <description> 访问控制，控制谁可以将任务提交到该队列,*表示任何人 </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
    <value>*</value>
    <description> 访问控制，控制谁可以管理(包括提交和取消)该队列的任务，*表示任何人 </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
    <value>*</value>
    <description> 指定哪个用户可以提交配置任务优先级 </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime
    </name>
    <value>-1</value>
    <description> hive 队列中任务的最大生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。 </description>
</property>
<property>
    <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
    <value>-1</value>
    <description> hive 队列中任务的默认生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。 </description>
</property>
```

(2) 分发配置文件

```shell
xsync /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml
```

(3)重启Hadoop 集群

**2）测试新队列**

（1）提交一个 MR 任务，并指定队列为 hive

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples -3.1.3.jar pi -Dmapreduce.job.queuename=hive 1 1
```

（2）查看 ResourceManager 的 web 页面，观察任务被提交到的队列

![webUi](https://gitee.com/zhdoop/blogImg/raw/master/img/增加hive队列webui.PNG)

#### 3.1.7 创建数据库

1）启动 hive

```shell
bin/hive
```

2）显示数据库

```sql
hive (default)> show databases;
```

3）创建数据库

```sql
hive (default)> create database gmall;
```

4）使用数据库

```shell
hive (default)> use gmall;
```

### 3.2 ODS层（用户行为数据）

#### 3.2.1 创建日志表 ods_log

![日志表ods_log](https://gitee.com/zhdoop/blogImg/raw/master/img/ODS层创建日志表.PNG)

1) 创建支持lzo 压缩的分区表

```sql
hive (gmall)>
drop table if exists ods_log;
CREATE EXTERNAL TABLE ods_log (`line` string) 
PARTITIONED BY (`dt` string) -- 按照时间创建分区 
STORED AS -- 指定存储方式，读数据采用LzoTextInputFormat； 
INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat' 
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION '/warehouse/gmall/ods/ods_log' -- 指定数据在 hdfs 上的存储位置 ;
```

说明 Hive 的 LZO 压缩：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LZO

2）加载数据

```sql
hive (gmall)>
load data inpath '/origin_data/gmall/log/topic_log/2020-06-14' 
into table ods_log partition(dt='2020-06-14');
```

注意：时间格式都配置成 YYYY-MM-DD 格式，这是 Hive 默认支持的时间格式

3）查看是否加载成功

```sql
hive (gmall)> select * from ods_log limit 2;
```

4）为 lzo 压缩文件创建索引

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20. jar com.hadoop.compression.lzo.DistributedLzoIndexer -Dmapreduce.job.queuename=hive /warehouse/gmall/ods/ods_log/dt=2020-06-14
```

