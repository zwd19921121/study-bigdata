# Spark SQL

## 1 Spark SQL 概述

本章从整体来了解 Spark SQL.

### 1.1 什么是Spark SQL

Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块.

与基本的 Spark RDD API 不同, Spark SQL 的抽象数据类型为 Spark 提供了关于数据结构和正在执行的计算的更多信息.

在内部, Spark SQL 使用这些额外的信息去做一些额外的优化.

有多种方式与 Spark SQL 进行交互, 比如: SQL 和 Dataset API. 当计算结果的时候, 使用的是相同的执行引擎, 不依赖你正在使用哪种 API 或者语言.

这种统一也就意味着开发者可以很容易在不同的 API 之间进行切换, 这些 API 提供了最自然的方式来表达给定的转换.

我们已经学习了 Hive，它是将 Hive SQL 转换成 MapReduce 然后提交到集群上执行，大大简化了编写 MapReduce 的程序的复杂性，

由于 MapReduce 这种计算模型执行效率比较慢, 所以 Spark SQL 的应运而生，它是将 Spark SQL 转换成 RDD，然后提交到集群执行，执行效率非常快！

Spark SQL 它提供了2个编程抽象, 类似 Spark Core 中的 **RDD**

*1.* **DataFrame**

*2.* **DataSet**

### 1.2 Spark SQL特点

#### 1.2.1 Integrated（易整合）

无缝的整合了 SQL 查询和 Spark 编程.

#### 1.2.2 Uniform Data Access(统一访问数据访问方式)

使用相同的方式连接不同的数据源.

#### 1.2.3 Hive Integration（集成Hive）

在已有的仓库上直接运行 SQL 或者 HiveQL

#### 1.2.4 Standard Connectivity（标准的连接方式）

通过 JDBC 或者 ODBC 来连接

### 1.3 什么是 DataFrame

与RDD 类似，DataFrame 也是一个分布式容器。

然而DataFrame 更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。

同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。

从API易用性的角度上看，DataFrame API提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。

![RDD与DataFrame](https://gitee.com/zhdoop/blogImg/raw/master/img/RDD与DataFrame.PNG)

上图直观地体现了**DataFrame**和**RDD**的区别。

左侧的**RDD[Person]**虽然以**Person**为类型参数，但Spark框架本身不了解**Person**类的内部结构。

而右侧的**DataFrame**却提供了详细的结构信息，使得 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。

**DataFrame**是为数据提供了**Schema**的视图。可以把它当做数据库中的一张表来对待，

**DataFrame**也是懒执行的。

性能上比 **RDD**要高，主要原因： 优化的执行计划：查询计划通过Spark catalyst optimiser进行优化。比如下面一个例子:

![DataFrame做的优化](https://gitee.com/zhdoop/blogImg/raw/master/img/DataFrame做的优化.PNG)

为了说明查询优化，我们来看上图展示的人口数据分析的示例。图中构造了两个**DataFrame**，将它们**join**之后又做了一次**filter**操作。

如果原封不动地执行这个执行计划，最终的执行效率是不高的。因为**join**是一个代价较大的操作，也可能会产生一个较大的数据集。

如果我们能将**filter**下推到 **join**下方，先对**DataFrame**进行过滤，再**join**过滤后的较小的结果集，便可以有效缩短执行时间。

而Spark SQL的查询优化器正是这样做的。简而言之，逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。

![性能对比图](https://gitee.com/zhdoop/blogImg/raw/master/img/DataFrame与RDD性能对比图.PNG)

### 1.4 什么是DataSet

1. 是**DataFrame API**的一个扩展，是 SparkSQL 最新的数据抽象(1.6新增)。
2. 用户友好的API风格，既具有类型安全检查也具有**DataFrame**的查询优化特性。
3. **Dataset**支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。
4. 样例类被用来在**DataSet**中定义数据的结构信息，样例类中每个属性的名称直接映射到**DataSet**中的字段名称。
5. **DataFrame**是**DataSet**的特列，**DataFrame=DataSet[Row]** ，所以可以通过**as**方法将**DataFrame**转换为**DataSet**。**Row**是一个类型，跟**Car、Person**这些的类型一样，所有的表结构信息都用**Row**来表示。
6. **DataSet**是强类型的。比如可以有**DataSet[Car]**，**DataSet[Person]**.
7. **DataFrame**只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个**String**进行减法操作，在执行的时候才报错，而**DataSet**不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟**JSON**对象和类对象之间的类比。

## 2 Spark SQL 编程

本章重点学习如何使用 **DataFrame**和**DataSet**进行编程. 已经他们之间的关系和转换.

关于具体的 SQL 书写不是本章的重点.

### 2.1 SparkSession

在老的版本中，SparkSQL 提供两种 SQL 查询起始点：一个叫**SQLContext**，用于Spark 自己提供的 SQL 查询；一个叫 **HiveContext**，用于连接 Hive 的查询。

从2.0开始, **SparkSession**是 Spark 最新的 SQL 查询起始点，实质上是**SQLContext**和**HiveContext**的组合，所以在**SQLContext**和**HiveContext**上可用的 API 在**SparkSession**上同样是可以使用的。

**SparkSession**内部封装了**SparkContext**，所以计算实际上是由**SparkContext**完成的。

当我们使用 spark-shell 的时候, spark 会自动的创建一个叫做**spark**的**SparkSession**, 就像我们以前可以自动获取到一个**sc**来表示**SparkContext**

### 2.2 使用DataFrame进行编程

首先学习 **DataFrame**相关的知识.

Spark SQL 的 **DataFrame** API 允许我们使用 **DataFrame** 而不用必须去注册临时表或者生成 SQL 表达式.

**DataFrame** API 既有 **transformation**操作也有**action**操作. **DataFrame**的转换从本质上来说更具有关系, 而 **DataSet** API 提供了更加函数式的 API

#### 2.2.1 创建 DataFrame

有了 **SparkSession** 之后, 通过 **SparkSession**有 3 种方式来创建**DataFrame**:

1. 通过Spark数据源创建
2. 通过已知的RDD来创建
3. 通过查询一个Hive表来创建

##### 2.2.1.1 通过Spark 数据源创建

Spark 支持的数据源：

```scala
scala>saprk.read
csv jdbc load options parquent table textFile format json option orc schema text
```

```scala
//读取json文件
scala> val df = spark.read.json("/opt/module/sparklocal/examples/src/main/resources/employees.json")
df: org.apache.spark.sql.DataFrame = [name: string, salary: bigint]
// 展示结果
scala> df.show
+-------+------+
|   name|salary|
+-------+------+
|Michael|  3000|
|   Andy|  4500|
| Justin|  3500|
|  Berta|  4000|
+-------+------+
```

##### 2.2.1.2通过RDD进行转换

后面章节专门讨论。

##### 2.2.1.3 通过查询 Hive 表创建

后面章节专门讨论。

#### 2.2.2 DataFrame 语法风格

##### 2.2.2.1 SQL语法风格（主要）

SQL 语法风格是指我们查询数据的时候使用SQL语句来查询。

这种风格的查询必须要有临时视图或者全局视图来辅助。

```shell
scala> val df = spark.read.json("/opt/module/spark-local/examples/src/main/resources/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.createOrReplaceTempView("people")

scala> spark.sql("select * from people").show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
```

注意：

1. 临时视图只能在当前 Session 有效，在新的Session中无效。
2. 可以创建全局视图，访问全局视图需要全路径：如global_temp.xxx

```scala
scala> val df = spark.read.json("/opt/module/spark-local/examples/src/main/resources/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.createGlobalTempView("people")
scala> spark.sql("select * from global_temp.people")
res31: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> res31.show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+


scala> spark.newSession.sql("select * from global_temp.people")
res33: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> res33.show
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
```

##### 2.2.2.2 DSL 语法风格（了解）

**DataFrame**提供一个特定领域语言(domain-specific language, DSL)去管理结构化的数据. 可以在 Scala, Java, Python 和 R 中使用 DSL。

使用DSL语法风格不必去创建临时视图了。

###### 2.2.2.2.1 查看Schema信息

```scala
scala> val df = spark.read.json("/opt/module/spark-local/examples/src/main/resources/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.printSchema
root
|-- age: long (nullable = true)
|-- name: string (nullable = true)
```

###### 2.2.2.2.2 使用DSL查询

1. 只查询 name 列数据

   ```scala
   scala> df.select($"name").show
   +-------+
   |   name|
   +-------+
   |Michael|
   |   Andy|
   | Justin|
   +-------+
   
   scala> df.select("name").show
   +-------+
   |   name|
   +-------+
   |Michael|
   |   Andy|
   | Justin|
   +-------+
   ```

2. 查询name和age

```scala
scala> df.select("name", "age").show
+-------+----+
|   name| age|
+-------+----+
|Michael|null|
|   Andy|  30|
| Justin|  19|
+-------+----+
```

3. 查询name 和 age + 1

   ```shell
   scala> df.select($"name", $"age" + 1).show
   +-------+---------+
   |   name|(age + 1)|
   +-------+---------+
   |Michael|     null|
   |   Andy|       31|
   | Justin|       20|
   +-------+---------+
   ```

   注意：设计到运算的时候，每列都必须使用$

4. 查询 age 大于 20 的数据

   ```scala
   scala> df.filter($"age" > 21).show
   +---+----+
   |age|name|
   +---+----+
   | 30|Andy|
   +---+----+
   ```

5.  按照 age 分组，查看数据条数

   ```scala
   scala> df.groupBy("age").count.show
   +----+-----+
   | age|count|
   +----+-----+
   |  19|    1|
   |null|    1|
   |  30|    1|
   +----+-----+
   ```

#### 2.2.3 RDD 和 DataFrame 的交互

**1. 从 RDD到DataFrame**

​	涉及到RDD，DataFrame ，DataSet 之间的操作时，需要导入:import spark.implicits._ 这里的spark 不是包名，而是表示SparkSession 的那个对象，所以必须先创建 SparkSession 对象再导入，implicits 是一个内部 object，首先创建一个RDD。

```scala
scala> val rdd1 = sc.textFile("/opt/module/spark-local/examples/src/main/resources/people.txt")
rdd1: org.apache.spark.rdd.RDD[String] = /opt/module/spark-local/examples/src/main/resources/people.txt MapPartitionsRDD[10] at textFile at <console>:24
```

**手动转换**

```scala
scala> val rdd2 = rdd1.map(line => { val paras = line.split(", "); (paras(0), paras(1).toInt)})
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[11] at map at <console>:26

// 转换为 DataFrame 的时候手动指定每个数据字段名
scala> rdd2.toDF("name", "age").show
+-------+---+
|   name|age|
+-------+---+
|Michael| 29|
|   Andy| 30|
| Justin| 19|
+-------+---+
```

**通过样例类反射转换**(最常用)

1. 创建样例类

   ```scala
   scala> case class People(name :String, age: Int)
   defined class People
   ```

2. 使用样例把 RDD 转换成 DataFrame

   ```scala
   scala> val rdd2 = rdd1.map(line => { val paras = line.split(", "); People(paras(0), paras(1).toInt) })
   rdd2: org.apache.spark.rdd.RDD[People] = MapPartitionsRDD[6] at map at <console>:28
   
   scala> rdd2.toDF.show
   +-------+---+
   |   name|age|
   +-------+---+
   |Michael| 29|
   |   Andy| 30|
   | Justin| 19|
   +-------+---+
   ```

**通过 API 的方式转换（了解）**

```scala
package day05

import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

object DataFrameDemo2 {
    def main(args: Array[String]): Unit = {

        val spark: SparkSession = SparkSession.builder()
            .master("local[*]")
            .appName("Word Count")
            .getOrCreate()
        val sc: SparkContext = spark.sparkContext
        val rdd: RDD[(String, Int)] = sc.parallelize(Array(("lisi", 10), ("zs", 20), ("zhiling", 40)))
        // 映射出来一个 RDD[Row], 因为 DataFrame其实就是 DataSet[Row]
        val rowRdd: RDD[Row] = rdd.map(x => Row(x._1, x._2))
        // 创建 StructType 类型
        val types = StructType(Array(StructField("name", StringType), StructField("age", IntegerType)))
        val df: DataFrame = spark.createDataFrame(rowRdd, types)
        df.show

    }
}
```

**2. 从 DataFrame 到 RDD**

直接调用DataFrame 的 rdd 方法就完成了转换

```shell
scala> val df = spark.read.json("/opt/module/spark-local/examples/src/main/resources/people.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> val rdd = df.rdd
rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[6] at rdd at <console>:25

scala> rdd.collect
res0: Array[org.apache.spark.sql.Row] = Array([null,Michael], [30,Andy], [19,Justin])
```

说明：

得到的RDD 中存储的数据类型是:Row

### 2.3 使用 DataSet 进行编程

DataSet 和 RDD 类似，但是 DataSet 没有使用 java序列化或者Kryo 序列化，而是使用一种专门的编码器去序列化对象，然后在网络上处理或者传输。

虽然编码器和标准序列化都负责将对象转换成字节，但是编码器是动态生成的代码，使用的格式允许Spark执行许多操作，如过滤，排序和哈希，而无需将字节反序列化回对象。

DataSet 是具有强类型的数据集合，需要提供对应的类型信息。

#### 2.3.1 创建DataSet

1. 使用样例类的序列得到 DataSet

   ```scala
   scala> case class Person(name: String, age: Int)
   defined class Person
   // 为样例类创建一个编码器
   scala> val ds = Seq(Person("lisi", 20), Person("zs", 21)).toDS
   ds: org.apache.spark.sql.Dataset[Person] = [name: string, age: int]
   scala> ds.show
   +----+---+
   |name|age|
   +----+---+
   |lisi| 20|
   | zs| 21 |
   +----+---+
   ```

2. 使用基本的序列得到 DataSet

   ```scala
   // 基本类型的编码被自动创建. importing spark.implicits._
   scala> val ds = Seq(1,2,3,4,5,6).toDS
   ds: org.apache.spark.sql.Dataset[Int] = [value: int]
   scala> ds.show
   +-----+
   |value|
   +-----+
   | 1|
   | 2|
   | 3|
   | 4|
   | 5|
   | 6|
   +-----+
   ```

   说明：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet。

#### 2.3.2 RDD和DataSet 的交互



 



'











