# Spark Core 项目实战

## 1 准备数据

本实战项目的数据是采集自电商的用户行为数据.

主要包含用户的 4 种行为: 搜索, 点击, 下单和支付.

数据格式如下, 不同的字段使用下划线分割开**_**:

![SparkCore项目实战数据格式](https://gitee.com/zhdoop/blogImg/raw/master/img/SparkCore项目实战数据格式.PNG)

**数据说明：**

1. 数据采用_分割字段
2. 每一行表示用户的一个行为，所以每一行只能是四种行为中的一种
3. 如果搜索关键字是null，表示这次不是搜索
4. 如果点击品类id和产品id是-1表示这次不是点击
5. 下单行为来说一次可以下单多个商品。所以品类id和产品id都是多个，id之间使用逗号分隔。如果本次不是下单行为，则他们相关数据用null来表示
6. 支付行为和下单行为类似

## 2 需求1 ：Top10 热门品类

### 2.1 需求1简介

品类是指的产品的分类，一些电商商品类分多级，咱们的项目中品类类只有一级。不同的公司可能对热门的定义不一样。我们按照每个品类的点击、下单、支付的量来统计热门品类。

### 2.2 需求1思路

#### 2.2.1 思路1

分别统计每个品类点击次数，下单次数和支付次数。

缺点：统计3次，需要启动3个job,每个job都有对原始数据遍历一次，非常耗时。

#### 2.2.2 思路2

最好的办法应该是遍历一次能够计算出来上述的3个指标。。

使用累加器可以达成我们的需求。

1. 遍历全部日志数据，根据品类id和操作类型分别累加。需要用到累加器

2. 定义累加器

3. 当碰到订单和支付业务的时候注意拆分字段才能得到品类的id

4. 遍历完成之后就得到每个品类id和操作类型的数量

5. 按照点击下单的顺序来排序

6. 取出top10

![RDD需求1思路](https://gitee.com/zhdoop/blogImg/raw/master/img/RDD需求1思路.PNG)

### 2.3 需求1具体实现

#### 2.3.1 用来封装用户行为的**bean**类

```scala
/**
  * 用户访问动作表
  *
  * @param date               用户点击行为的日期
  * @param user_id            用户的ID
  * @param session_id         Session的ID
  * @param page_id            某个页面的ID
  * @param action_time        动作的时间点
  * @param search_keyword     用户搜索的关键词
  * @param click_category_id  某一个商品品类的ID
  * @param click_product_id   某一个商品的ID
  * @param order_category_ids 一次订单中所有品类的ID集合
  * @param order_product_ids  一次订单中所有商品的ID集合
  * @param pay_category_ids   一次支付中所有品类的ID集合
  * @param pay_product_ids    一次支付中所有商品的ID集合
  * @param city_id            城市 id
  */
case class UserVisitAction(date: String,
                           user_id: Long,
                           session_id: String,
                           page_id: Long,
                           action_time: String,
                           search_keyword: String,
                           click_category_id: Long,
                           click_product_id: Long,
                           order_category_ids: String,
                           order_product_ids: String,
                           pay_category_ids: String,
                           pay_product_ids: String,
                           city_id: Long)
case class CategoryCountInfo(categoryId: String,
                             clickCount: Long,
                             orderCount: Long,
                             payCount: Long)
```

#### 2.3.2 定义用到的累加器

需要统计每个品类的点击量, 下单量和支付量, 所以我们在累加器中使用 Map 来存储这些数据: Map(cid, “click”-> 100, cid, “order”-> 50, ….)

```scala
import org.apache.spark.util.AccumulatorV2

import scala.collection.mutable

class MapAccumulator extends AccumulatorV2[(String, String), mutable.Map[(String, String), Long]] {
    val map: mutable.Map[(String, String), Long] = mutable.Map[(String, String), Long]()
    
    override def isZero: Boolean = map.isEmpty
    
    override def copy(): AccumulatorV2[(String, String), mutable.Map[(String, String), Long]] = {
        val newAcc = new MapAccumulator
        map.synchronized {
            newAcc.map ++= map
        }
        newAcc
    }
    
    override def reset(): Unit = map.clear
    
    
    override def add(v: (String, String)): Unit = {
        map(v) = map.getOrElseUpdate(v, 0) + 1
    }
    
    // otherMap: (1, click) -> 20 this: (1, click) -> 10 thisMap: (1,2) -> 30
    // otherMap: (1, order) -> 5 thisMap: (1,3) -> 5
    override def merge(other: AccumulatorV2[(String, String), mutable.Map[(String, String), Long]]): Unit = {
        val otherMap: mutable.Map[(String, String), Long] = other.value
        otherMap.foreach {
            kv => map.put(kv._1, map.getOrElse(kv._1, 0L) + kv._2)
        }
    }
    override def value: mutable.Map[(String, String), Long] = map
}
```

#### 2.3.3 整体入口

```scala
import com.atguigu.practice.app.bean.{CategoryCountInfo, UserVisitAction}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD

/**
  * Author lzc
  * Date 2019/8/9 10:56 AM
  */
object PracticeApp {
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setAppName("Practice").setMaster("local[2]")
        val sc = new SparkContext(conf)
        // 1. 读取文件中的数据
        val lineRDD: RDD[String] = sc.textFile("/Users/lzc/Desktop/user_visit_action.txt")
        // 2. 类型调整
        val userVisitActionRDD: RDD[UserVisitAction] = lineRDD.map(line => {
            val splits: Array[String] = line.split("_")
            UserVisitAction(
                splits(0),
                splits(1).toLong,
                splits(2),
                splits(3).toLong,
                splits(4),
                splits(5),
                splits(6).toLong,
                splits(7).toLong,
                splits(8),
                splits(9),
                splits(10),
                splits(11),
                splits(12).toLong)
        })
        
        // 需求 1
        val categoryTop10: List[CategoryCountInfo] = CategoryTop10App.statCategoryTop10(sc, userVisitActionRDD)
        println(CategoryCountInfoList)
        
        sc.stop()
    }
}
```

#### 2.3.4 需求1的具体代码

```scala
import com.atguigu.practice.app.acc.MapAccumulator
import com.atguigu.practice.app.bean.{CategoryCountInfo,UserVisitAction}
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD

import scala.collection.mutable

object CategoryTop10App {
    def statCategoryTop10(sc: SparkContext, userVisitActionRDD: RDD[UserVisitAction]): List[CategoryCountInfo] = {
        // 1. 注册累加器
        val acc = new MapAccumulator
        sc.register(acc, "CategoryActionAcc")
        
        // 2. 遍历日志
        userVisitActionRDD.foreach {
            visitAction => {
                if (visitAction.click_category_id != -1) {
                    acc.add((visitAction.click_category_id.toString, "click"))
                } else if (visitAction.order_category_ids != "null") {
                    visitAction.order_category_ids.split(",").foreach {
                        oid => acc.add((oid, "order"))
                    }
                } else if (visitAction.pay_category_ids != "null") {
                    visitAction.pay_category_ids.split(",").foreach {
                        pid => acc.add((pid, "pay"))
                    }
                }
            }
        }
        
        // 3. 遍历完成之后就得到每个每个品类 id 和操作类型的数量. 然后按照 CategoryId 进行进行分组
        val actionCountByCategoryIdMap: Map[String, mutable.Map[(String, String), Long]] = acc.value.groupBy(_._1._1)
        
        // 4. 转换成 CategoryCountInfo 类型的集合, 方便后续处理
        val categoryCountInfoList: List[CategoryCountInfo] = actionCountByCategoryIdMap.map {
            case (cid, actionMap) => CategoryCountInfo(
                cid,
                actionMap.getOrElse((cid, "click"), 0),
                actionMap.getOrElse((cid, "order"), 0),
                actionMap.getOrElse((cid, "pay"), 0)
            )
        }.toList
        
        // 5. 按照 点击 下单 支付 的顺序降序来排序
        val sortedCategoryInfoList: List[CategoryCountInfo] = categoryCountInfoList.sortBy(info => (info.clickCount, info.orderCount, info.payCount))(Ordering.Tuple3(Ordering.Long.reverse, Ordering.Long.reverse, Ordering.Long.reverse))
        
        // 6. 截取前 10
                val top10: List[CategoryCountInfo] = sortedCategoryInfoList.take(10)
        // 7. 返回 top10 品类 id
        top10
    }
}
```

## 3 需求2：Top10 热门品类中每个品类的Top10活跃Session统计

### 3.1 需求分析

对于排名前 10 的品类，分别获取每个品类点击次数排名前 10 的 sessionId。(注意: 这里我们只关注点击次数, 不关心下单和支付次数)

这个就是说，对于 top10 的品类，每一个都要获取对它点击次数排名前 10 的 sessionId。

这个功能，可以让我们看到，对某个用户群体最感兴趣的品类，各个品类最感兴趣最典型的用户的 session 的行为。

### 3.2 思路

1. 过滤出来 category Top10的日志

2. 需要用到需求1的结果，然后需要得到categoryId 就可以了

3. 转换结果为 RDD[(categoryId,sessionId),1] 然后统计数量 => RDD[(categoryId,sessionId),count]

4. 统计每个品类 top10 =》 RDD[categoryId,(sessionId,count)] => RDD[categoryId,Iterable[(sessionId,count)]]

5. 对每个 Iterable[(sessionId,count)] 进行排序，并取每个 Iterable 的前10

6. 把数据封装到 CategorySession 中

   ![ ](C:\Users\zwd\AppData\Roaming\Typora\typora-user-images\image-20201029094437958.png)

### 3.3 具体代码实现

#### 3.3.1 bean 类

**CategorySession** 类

封装最终写入到数据库的数据

```scala
case class CategorySession(categoryId: String,
                           sessionId: String,
                           clickCount: Long)
```

#### 3.3.2 具体实现

```scala
package com.atguigu.practice.app

import com.atguigu.practice.app.bean.{CategoryCountInfo, CategorySession, UserVisitAction}
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD


object CategorySessionApp {
    def statCategoryTop10Session(sc: SparkContext, userVisitActionRDD: RDD[UserVisitAction], categoryTop10: List[CategoryCountInfo]) = {
        // 1. 得到top10的品类的id
        val categoryIdTop10: List[String] = categoryTop10.map(_.categoryId)
        // 2. 过去出来只包含 top10 品类id的那些用户行为
        val filteredUserVisitActionRDD: RDD[UserVisitAction] = userVisitActionRDD.filter(UserVisitAction => {categoryIdTop10.contains(UserVisitAction.click_category_id.toString)
        })
        // 3. 聚合操作
        //  => RDD[(品类id, sessionId))] map
        //    => RDD[(品类id, sessionId), 1)]
        val categorySessionOne: RDD[((Long, String), Int)] = filteredUserVisitActionRDD
            .map(userVisitAction => ((userVisitAction.click_category_id, userVisitAction.session_id), 1))
        // RDD[(品类id, sessionId), count)]
        val categorySessionCount: RDD[(Long, (String, Int))] =
            categorySessionOne.reduceByKey(_ + _).map {
                case ((cid, sid), count) => (cid, (sid, count))
            }
        // 4. 按照品类 id 进行分组
        // RDD[品类id, Iterator[(sessionId, count)]]
        val categorySessionCountGrouped: RDD[(Long, Iterable[(String, Int)])] = categorySessionCount.groupByKey
        
        // 5. 排序取前 10
        val categorySessionRDD: RDD[CategorySession] = categorySessionCountGrouped.flatMap {
            case (cid, it) => {
                val list: List[(String, Int)] = it.toList.sortBy(_._2)(Ordering.Int.reverse).take(10)
                val result: List[CategorySession] = list.map {
                    case (sid, count) => CategorySession(cid.toString, sid, count)
                }
                result
            }
        }
        categorySessionRDD.collect.foreach(println)
    }
}
/*
1. 得到top10的品类的id

2. 过去出来只包含 top10 品类id的那些用户行为

3. 分组计算
    => RDD[(品类id, sessionId))] map
    => RDD[(品类id, sessionId), 1)]  reduceByKey
    => RDD[(品类id, sessionId), count)]    map
    => RDD[品类id, (sessionId, count)]     groupByKey
    RDD[品类id, Iterator[(sessionId, count)]]
 */
```

#### 3.3.3 前面的实现存在的问题

下面的代码可能存在的问题:

```scala
 // 5. 排序取前 10
val categorySessionRDD: RDD[CategorySession] = categorySessionCountGrouped.flatMap {
    case (cid, it) => {
        val list: List[(String, Int)] = it.toList.sortBy(_._2)(Ordering.Int.reverse).take(10)
        val result: List[CategorySession] = list.map {
            case (sid, count) => CategorySession(cid.toString, sid, count)
        }
        result
    }
}
```

上面的操作中, 有一个操作是把迭代器中的数据转换成**List**之后再进行排序, 这里存在内存溢出的可能. 如果迭代器的数据足够大, 当转变成 List 的时候, 会把这个迭代器的所有数据都加载到内存中, 所以有可能造成内存的溢出.

前面的排序是使用的 Scala 的排序操作, 由于 scala 排序的时候需要把数据全部加载到内存中才能完成排序, 所以理论上都存在内存溢出的风险.

如果使用 RDD 提供的排序功能, 可以避免内存溢出的风险, 因为 RDD 的排序需要 shuffle, 是采用了内存+磁盘来完成的排序.

#### 3.3.4 解决方案1

使用 RDD 的排序功能, 但是由于 RDD 排序是对所有的数据整体排序, 所以一次只能针对一个 CategoryId 进行排序操作

参考代码如下：

```scala
def statCategoryTop10Session_1(sc: SparkContext, userVisitActionRDD: RDD[UserVisitAction], categoryTop10: List[CategoryCountInfo]) = {
    // 1. 得到top10的品类的id
    val categoryIdTop10: List[String] = categoryTop10.map(_.categoryId)
    // 2. 过去出来只包含 top10 品类id的那些用户行为
    val filteredUserVisitActionRDD: RDD[UserVisitAction] = userVisitActionRDD.filter(UserVisitAction => {
        categoryIdTop10.contains(UserVisitAction.click_category_id.toString)
    })
    // 3. 聚合操作
    //  => RDD[(品类id, sessionId))] map
    //    => RDD[(品类id, sessionId), 1)]
    val categorySessionOne: RDD[((Long, String), Int)] = filteredUserVisitActionRDD
        .map(userVisitAction => ((userVisitAction.click_category_id, userVisitAction.session_id), 1))
    // RDD[(品类id, sessionId), count)]
    val categorySessionCount: RDD[(Long, (String, Int))] =
        categorySessionOne.reduceByKey(_ + _).map {
            case ((cid, sid), count) => (cid, (sid, count))
        }
    // 4. 每个品类 id 排序取前 10的 session
    categoryIdTop10.foreach(cid => {
        // 针对某个具体的 CategoryId, 过滤出来只只包含这个CategoryId的 RDD, 然后整体j降序p排列
        val top10: Array[CategorySession] = categorySessionCount
            .filter(_._1 == cid.toLong)
            .sortBy(_._2._2, ascending = false)
            .take(10)
            .map {
                case (cid, (sid, count)) => CategorySession(cid.toString, sid, count)
            }
        top10.foreach(println)
        
    })
    
}
```

#### 3.3.5 解决方案2

方案 1 解决了内存溢出的问题, 但是也有另外的问题: 提交的 job 比较多, 有一个品类 id 就有一个 job, 在本案例中就有了 10 个 job.

有没有更加好的方案呢?

可以把同一个品类的数据都进入到同一个分区内, 然后对每个分区的数据进行排序!

需要用到自定义分区器.

##### **自定义分区器**

```scala
class MyPartitioner(categoryIdTop10: List[String]) extends Partitioner {
    // 给每个 cid 配一个分区号(使用他们的索引就行了)
    private val cidAndIndex: Map[String, Int] = categoryIdTop10.zipWithIndex.toMap
    
    override def numPartitions: Int = categoryIdTop10.size
    
    override def getPartition(key: Any): Int = {
        key match {
            case (cid: Long, _) => cidAndIndex(cid.toString)
        }
    }
}
```

##### **CategorySession修改**

```scala
case class CategorySession(categoryId: String,
                           sessionId: String,
                           clickCount: Long) extends Ordered[CategorySession] { // 实现Ordered接口, 让对象具有比较的能力
    override def compare(that: CategorySession): Int = {
        if (this.clickCount <= that.clickCount) 1
        else -1
    }
}
```

**具体方法**

```scala
def statCategoryTop10Session_2(sc: SparkContext, userVisitActionRDD: RDD[UserVisitAction], categoryTop10: List[CategoryCountInfo]) = {
    // 1. 得到top10的品类的id
    val categoryIdTop10: List[String] = categoryTop10.map(_.categoryId)
    // 2. 过去出来只包含 top10 品类id的那些用户行为
    val filteredUserVisitActionRDD: RDD[UserVisitAction] = userVisitActionRDD.filter(UserVisitAction => {
        categoryIdTop10.contains(UserVisitAction.click_category_id.toString)
    })
    // 3. 聚合操作
    //  => RDD[(品类id, sessionId))] map
    //    => RDD[(品类id, sessionId), 1)]
    val categorySessionOne: RDD[((Long, String), Int)] = filteredUserVisitActionRDD
        .map(userVisitAction => ((userVisitAction.click_category_id, userVisitAction.session_id), 1))
    // RDD[(品类id, sessionId), count)]  在 reduceByKey 的时候指定分区器
    val categorySessionCount: RDD[CategorySession] = categorySessionOne
        .reduceByKey(new MyPartitioner(categoryIdTop10), _ + _) //  指定分区器  (相比以前有变化)
        .map {
        case ((cid, sid), count) => CategorySession(cid.toString, sid, count)
    }
    
    // 4. 对每个分区内的数据排序取前 10(相比以前有变化)
    val categorySessionRDD: RDD[CategorySession] = categorySessionCount.mapPartitions(it => {
        
        // 这个时候也不要把 it 变化 list 之后再排序, 否则仍然会有可能出现内存溢出.
        // 我们可以把数据存储到能够自动排序的集合中 比如 TreeSet 或者 TreeMap 中, 并且永远保持这个集合的长度为 10
        // 让TreeSet默认按照 count 的降序排列, 需要让CategorySession实现 Ordered 接口(Comparator)
        var top10: mutable.TreeSet[CategorySession] = mutable.TreeSet[CategorySession]()
        
        it.foreach(cs => {
            top10 += cs // 把 CategorySession 添加到 TreeSet 中
            if (top10.size > 10) { // 如果 TreeSet 的长度超过 10, 则移除最后一个
                top10 = top10.take(10)
            }
        })
        top10.toIterator
    })
    categorySessionRDD.collect.foreach(println)
    
}
```

## 4 需求3：页面单跳转化率统计

### 4.1 需求简介

计算页面单跳转化率，什么是页面单跳转换率，比如一个用户在一次 Session 过程中访问的页面路径 **3,5,7,9,10,21**，那么页面 **3** 跳到页面 **5** 叫一次单跳，**7-9** 也叫一次单跳，那么单跳转化率就是要统计页面点击的概率

比如：计算 **3-5** 的单跳转化率，先获取符合条件的 **Session** 对于页面 3 的访问次数（PV）为 A，然后获取符合条件的 Session 中访问了页面 3 又紧接着访问了页面 5 的次数为 B，那么 B/A 就是 3-5 的页面单跳转化率

![页面转化率](https://gitee.com/zhdoop/blogImg/raw/master/img/页面转换率截图.PNG)

产品经理和运营总监，可以根据这个指标，去尝试分析，整个网站，产品，各个页面的表现怎么样，是不是需要去优化产品的布局；吸引用户最终可以进入最后的支付页面。

数据分析师，可以此数据做更深一步的计算和分析。

企业管理层，可以看到整个公司的网站，各个页面的之间的跳转的表现如何，可以适当调整公司的经营战略或策略。

在该模块中，需要根据查询对象中设置的 Session 过滤条件，先将对应得 Session 过滤出来，然后根据查询对象中设置的页面路径，计算页面单跳转化率，比如查询的页面路径为：3、5、7、8，那么就要计算 3-5、5-7、7-8 的页面单跳转化率。

需要注意的一点是，页面的访问时有先后的，要做好排序。

### 4.2 思路分析

1. 读取到规定的页面
2. 过滤出来规定页面的日志记录，并统计出来每个页面的访问次数**countByKey** 是行动算子 **reduceByKey** 是转换算子
3. 明确哪些页面需要计算跳转次数 **1-2**, **2-3**, **3-4** …
4. 按照 session 统计所有页面的跳转次数, 并且需要按照时间升序来排序
5. 按照 session 分组, 然后并对每组内的 **UserVisitAction** 进行排序
6. 转换访问流水
7. 过滤出来和统计目标一致的跳转
8. 统计跳转次数
9. 计算跳转率

![页面转化率实现思路](https://gitee.com/zhdoop/blogImg/raw/master/img/页面转化率实现思路.PNG)

### 4.3 具体业务实现

```scala
package com.atguigu.practice.app

import java.text.DecimalFormat

import com.atguigu.practice.app.bean.UserVisitAction
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD


object PageConversionApp {
    def calcPageConversion(spark: SparkContext, userVisitActionRDD: RDD[UserVisitAction], targetPageFlow: String) = {
        // 1. 读取到规定的页面
        val pageFlowArr: Array[String] = targetPageFlow.split(",")
        val prePageFlowArr: Array[String] = pageFlowArr.slice(0, pageFlowArr.length - 1)
        val postPageFlowArr: Array[String] = pageFlowArr.slice(1, pageFlowArr.length)
        // 2. 过滤出来规定页面的日志记录, 并统计出来每个页面的访问次数   countByKey 是行动算子  reduceByKey 是转换算子
        val targetPageCount: collection.Map[Long, Long] = userVisitActionRDD
            .filter(uva => pageFlowArr.contains(uva.page_id.toString))
            .map(uva => (uva.page_id, 1L))
            .countByKey
        
        // 3. 明确哪些页面需要计算跳转次数 1-2  2-3 3-4 ...   (组合出来跳转流)
        val targetJumpPages: Array[String] = prePageFlowArr.zip(postPageFlowArr).map(t => t._1 + "-" + t._2)
        // 4. 按照 session 统计所有页面的跳转次数, 并且需要按照时间升序来排序
        // 4.1 按照 session 分组, 然后并对每组内的 UserVisitAction 进行排序
        val pageJumpRDD: RDD[String] = userVisitActionRDD.groupBy(_.session_id).flatMap {
            case (_, actions) => {
                val visitActions: List[UserVisitAction] = actions.toList.sortBy(_.action_time)
                // 4.2 转换访问流水
                val pre: List[UserVisitAction] = visitActions.slice(0, visitActions.length - 1)
                val post: List[UserVisitAction] = visitActions.slice(1, visitActions.length)
                // 4.3 过滤出来和统计目标一致的跳转
                pre.zip(post).map(t => t._1.page_id + "-" + t._2.page_id).filter(targetJumpPages.contains(_))
            }
        }
        
        // 5. 统计跳转次数  数据量已经很少了, 拉到驱动端计算
        val pageJumpCount: Array[(String, Int)] = pageJumpRDD.map((_, 1)).reduceByKey(_ + _).collect
        
        // 6. 计算跳转率
        
        val formatter = new DecimalFormat(".00%")
        // 转换成百分比
        val conversionRate: Array[(String, String)] = pageJumpCount.map {
            case (p2p, jumpCount) =>
                val visitCount: Long = targetPageCount.getOrElse(p2p.split("-").head.toLong, 0L)
                val rate: String = formatter.format(jumpCount.toDouble / visitCount)
                (p2p, rate)
        }
        conversionRate.foreach(println)
    }
}

/*
1. 读取到规定的页面
    例如: targetPageFlow:"1,2,3,4,5,6,7"

2. 过滤出来规定页面的日志记录 并统计出来每个页面的访问次数
    例如: 只需过滤出来1,2,3,4,5,6   第7页面不需要过滤

3. 计算页面跳转次数(肯定是按照每个 session 来统计)
    1->2  2->3 ...
    3.1 统计每个页面访问次数

4. 计算转化率
页面跳转次数 / 页面访问次数
    1->2/1 表示页面1到页面2的转化率

*/
```

### 4.4 执行

```scala
PageConversionApp.calcPageConversion(sc, userVisitActionRDD, "1,2,3,4,5,6")
```

执行结果

![执行结果](https://gitee.com/zhdoop/blogImg/raw/master/img/RDD需求3执行结果.PNG)