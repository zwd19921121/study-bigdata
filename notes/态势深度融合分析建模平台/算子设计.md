# 算子管理

## 1 数据输入算子

### 1.1 离线

离线算子的抽象类

```scala
abstract class DefaultOfflineSource(props : java.util.Properties, ssaProps : java.util.Properties) extends dfamp.source.DefaultInput {
  def get(spark : org.apache.spark.sql.SparkSession) : org.apache.spark.rdd.RDD[scala.Predef.String]
  def getInput(spark : org.apache.spark.sql.SparkSession) : org.apache.spark.rdd.RDD[com.alibaba.fastjson.JSONObject] = { /* compiled code */ }
}
```

#### 1.1.1 离线测试数据生成

##### 1.1.1.1 描述

根据输入的多个JSONObject 格式的字符串生成String 类型RDD的测试源，提取以'<font color=red>{</font>'开头，以<font color=red>}</font> 结尾的JSONObject 格式的字符串，支持任意分隔符，如换行、逗号等支持忽略前后的不相关内容，每条数据必须为JSONObject格式的字符串。

##### 1.1.1.2 配置列表

<table border="1">
  <tr>
    <th>配置项</th>
    <th>名称</th>
    <th>默认值</th>
    <th>类型</th>
    <th>描述</th>
  </tr>
  <tr>
    <td>class-name</td>
    <td>类名称</td>
    <td>dfamp.source.offline.TestOfflineSource</td>
    <td>readonly</td>
    <td></td>
  </tr>
  <tr>
    <td>test-text</td>
    <td>测试数据字符串</td>
    <td></td>
    <td>string</td>
    <td>根据输入的多个 JSONObject 格式的字符串生成 String 类型 RDD 的测试源<br/> 提取以 { 开头，以 } 结尾的 JSONObject 格式的字符串<br/> 支持任意分隔符，如换行、逗号等<br/> 支持忽略前后的不相关内容<br/> 每条数据必须为 JSONObject 格式的字符串</td>
  </tr>
   <tr>
    <td>execute-interval</td>
    <td>执行间隔</td>
    <td></td>
    <td>string</td>
    <td>只有一个输入算子的时候生效 可以为数字后面加后缀,忽略大小写 其中:ms 代表毫秒,s 代表秒,m 代表分钟,h 代表小时,d 代表天,w 代表周  无后缀时默认为 s</td>
  </tr>
   <tr>
    <td>schedule-execute-time</td>
    <td>计划执行时间</td>
    <td></td>
    <td>string</td>
    <td>只有一个输入算子的时候生效 格式为 yyyy-MM-dd HH:mm:ss  若设定执行时间未到,则程序会在设定时间开始执行 若设定执行时间已过,则程序会立即执行  若同时设定了执行间隔,则下次执行时间与计划执行时间的间隔将为执行间隔的倍数</td>
  </tr>
</table>
##### 1.1.1.3 源码分析

1. 初始化获取TEST_TEXT

```scala
override def initialize(sc: SparkContext): Unit = {
    TEST_TEXT = getRequiredPropBy("test-text")
}
```

2. 读取配置多个JSONObject 格式的字符串转换成数组，然后转换成String 类型的RDD
```scala
 override def get(spark: SparkSession): RDD[String] = {
     infoLog("执行算子: ", getAppID, " 读取测试数据!") //打印日志到mysql中
     val textAB = ArrayTool.jsonStringABFromText(TEST_TEXT) //将多个json字符串分割成数组
     if (textAB.nonEmpty) {
         spark.sparkContext.parallelize(textAB) //生成RDD
     } else {
         throw new RuntimeException("不能获取到 JSONObject 格式的测试数据!")
     }
}
```

#### 1.1.2 CSV 数据输入



### 1.2 在线

## 2 数据输出

## 3 计算算子

